"""Triplet extraction service using LLM."""
import json
from typing import List, Optional
from openai import OpenAI
from infra.config import settings
from models.document import Triplet, Chunk


class TripletExtractor:
    """Extract triplets from text using LLM."""
    
    def __init__(self):
        self.client = None
        self.provider = settings.ai_provider
        self.model = None
        
        if self.provider == "openai":
            if settings.openai_api_key:
                self.client = OpenAI(
                    api_key=settings.openai_api_key,
                    base_url=settings.openai_base_url
                )
                self.model = settings.openai_model
                print(f"âœ… ä½¿ç”¨ OpenAI APIï¼Œæ¨¡åž‹: {self.model}")
            else:
                print("âš ï¸  è­¦å‘Š: OPENAI_API_KEY æœªè®¾ç½®ï¼Œå°†ä½¿ç”¨ mock æ¨¡å¼")
                self.provider = "mock"
        
        elif self.provider == "ollama":
            try:
                # Ollamaä½¿ç”¨OpenAIå…¼å®¹çš„API
                self.client = OpenAI(
                    base_url=f"{settings.ollama_base_url}/v1",
                    api_key="ollama"  # Ollamaä¸éœ€è¦çœŸå®žçš„API key
                )
                self.model = settings.ollama_model
                print(f"âœ… ä½¿ç”¨ Ollamaï¼Œåœ°å€: {settings.ollama_base_url}ï¼Œæ¨¡åž‹: {self.model}")
            except Exception as e:
                print(f"âš ï¸  è­¦å‘Š: æ— æ³•è¿žæŽ¥åˆ° Ollama ({e})ï¼Œå°†ä½¿ç”¨ mock æ¨¡å¼")
                self.provider = "mock"
                self.client = None
        
        else:
            print("â„¹ï¸  ä½¿ç”¨ mock æ¨¡å¼è¿›è¡Œä¸‰å…ƒç»„æå–")
    
    def extract(self, chunk: Chunk) -> List[Triplet]:
        """
        Extract triplets from a text chunk.
        
        Args:
            chunk: Text chunk to extract from
            
        Returns:
            List of Triplet objects
        """
        print(f"\n{'='*80}")
        print(f"ðŸ” [çŸ¥è¯†æŠ½å–] å¼€å§‹å¤„ç†æ–‡æœ¬å— (chunk_id: {chunk.chunk_id})")
        print(f"ðŸ“„ æ–‡æœ¬é•¿åº¦: {len(chunk.text)} å­—ç¬¦")
        print(f"ðŸ“ æ–‡æœ¬é¢„è§ˆ: {chunk.text[:200]}...")
        
        if not self.client or self.provider == "mock":
            print(f"âš ï¸  [çŸ¥è¯†æŠ½å–] ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆæœªé…ç½® AI æœåŠ¡ï¼‰")
            # Mock mode: return empty list or simple extraction
            result = self._mock_extract(chunk)
            print(f"ðŸ“Š [çŸ¥è¯†æŠ½å–] Mock æ¨¡å¼æå–ç»“æžœ: {len(result)} ä¸ªä¸‰å…ƒç»„")
            return result
        
        prompt = self._build_prompt(chunk.text)
        raw_content = None  # åˆå§‹åŒ–å˜é‡ï¼Œç”¨äºŽé”™è¯¯å¤„ç†
        
        try:
            print(f"ðŸ¤– [AIè¯·æ±‚] Provider: {self.provider}, Model: {self.model}")
            print(f"ðŸ“¤ [AIè¯·æ±‚] å‘é€è¯·æ±‚åˆ° AI æœåŠ¡...")
            
            # æ ¹æ®providerè°ƒæ•´å‚æ•°
            completion_params = {
                "model": self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a knowledge extraction expert. Extract subject-predicate-object triplets from the given text. Return only valid JSON array."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.3,
            }
            
            # OpenAIæ”¯æŒresponse_formatï¼ŒOllamaå¯èƒ½ä¸æ”¯æŒ
            if self.provider == "openai":
                completion_params["response_format"] = {"type": "json_object"}
            
            response = self.client.chat.completions.create(**completion_params)
            
            # æ˜¾ç¤ºAIå“åº”çš„åŽŸå§‹å†…å®¹
            raw_content = response.choices[0].message.content
            print(f"ðŸ“¥ [AIå“åº”] æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(raw_content)} å­—ç¬¦")
            print(f"ðŸ“¥ [AIå“åº”] åŽŸå§‹å†…å®¹é¢„è§ˆ: {raw_content[:500]}...")
            
            # è§£æžJSONå“åº”
            result = json.loads(raw_content)
            raw_triplets = result.get("triplets", [])
            print(f"ðŸ“Š [AIå“åº”] è§£æžåˆ°åŽŸå§‹ä¸‰å…ƒç»„æ•°é‡: {len(raw_triplets)}")
            
            # æ˜¾ç¤ºåŽŸå§‹ä¸‰å…ƒç»„è¯¦æƒ…
            for idx, t in enumerate(raw_triplets[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   [{idx}] {t.get('subject', 'N/A')} - {t.get('predicate', 'N/A')} - {t.get('object', 'N/A')} (ç½®ä¿¡åº¦: {t.get('confidence', 0)})")
            if len(raw_triplets) > 5:
                print(f"   ... è¿˜æœ‰ {len(raw_triplets) - 5} ä¸ªä¸‰å…ƒç»„")
            
            # è¿‡æ»¤å’Œè½¬æ¢ä¸‰å…ƒç»„
            triplets = [
                Triplet(
                    subject=t.get("subject", ""),
                    predicate=t.get("predicate", ""),
                    object=t.get("object", ""),
                    confidence=t.get("confidence", 0.8),
                    evidence={
                        "docId": chunk.doc_id,
                        "chunkId": chunk.chunk_id,
                        "page": chunk.meta.get("page"),
                        "offset": chunk.meta.get("offset"),
                        "text": chunk.text[:200]  # Preview
                    },
                    doc_id=chunk.doc_id,
                    chunk_id=chunk.chunk_id
                )
                for t in raw_triplets
                if t.get("subject") and t.get("predicate") and t.get("object")
            ]
            
            # æ˜¾ç¤ºè¿‡æ»¤åŽçš„ç»“æžœ
            filtered_count = len(raw_triplets) - len(triplets)
            if filtered_count > 0:
                print(f"âš ï¸  [è¿‡æ»¤] è¿‡æ»¤æŽ‰ {filtered_count} ä¸ªæ— æ•ˆä¸‰å…ƒç»„ï¼ˆç¼ºå°‘å¿…è¦å­—æ®µï¼‰")
            
            print(f"âœ… [çŸ¥è¯†æŠ½å–] æˆåŠŸæå– {len(triplets)} ä¸ªæœ‰æ•ˆä¸‰å…ƒç»„")
            print(f"{'='*80}\n")
            
            return triplets
        except json.JSONDecodeError as e:
            print(f"âŒ [çŸ¥è¯†æŠ½å–] JSON è§£æžé”™è¯¯: {e}")
            if raw_content:
                print(f"ðŸ“¥ [AIå“åº”] åŽŸå§‹å“åº”å†…å®¹: {raw_content[:1000]}")
            return []
        except Exception as e:
            print(f"âŒ [çŸ¥è¯†æŠ½å–] æå–å¤±è´¥: {e}")
            import traceback
            print(f"ðŸ“‹ [é”™è¯¯è¯¦æƒ…] {traceback.format_exc()}")
            return []
    
    def _build_prompt(self, text: str) -> str:
        """Build bilingual extraction prompt for both Chinese and English."""
        return f"""You are a professional knowledge graph construction expert. Extract structured knowledge triplets (subject-relation-object) from the given text.
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æž„å»ºä¸“å®¶ã€‚è¯·ä»Žä»¥ä¸‹æ–‡æœ¬ä¸­æå–ç»“æž„åŒ–çš„çŸ¥è¯†ä¸‰å…ƒç»„ï¼ˆä¸»ä½“-å…³ç³»-å®¢ä½“ï¼‰ã€‚

**Text Content / æ–‡æœ¬å†…å®¹ï¼š**
{text}

**Task Requirements / ä»»åŠ¡è¦æ±‚ï¼š**
1. Identify core concepts, entities, and key information / è¯†åˆ«æ ¸å¿ƒæ¦‚å¿µã€å®žä½“å’Œå…³é”®ä¿¡æ¯
2. Extract semantic relationships between them / æå–å®ƒä»¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³»
3. Perform structured organization and optimization / è¿›è¡Œç»“æž„åŒ–æ•´ç†å’Œä¼˜åŒ–
4. Ensure knowledge practicality and accuracy / ç¡®ä¿çŸ¥è¯†å…·æœ‰å®žç”¨æ€§å’Œå‡†ç¡®æ€§
5. **Preserve original language**: Keep concepts in their original language (Chinese/English) / **ä¿ç•™åŽŸå§‹è¯­è¨€**ï¼šä¿æŒæ¦‚å¿µçš„åŽŸå§‹è¯­è¨€ï¼ˆä¸­è‹±æ–‡ï¼‰

**Relationship Types Guide / å…³ç³»ç±»åž‹æŒ‡å—ï¼š**
- `is_a` / `å®šä¹‰ä¸º`: A is a type/kind of B / A æ˜¯ B çš„ä¸€ç§/ä¸€ç±»
- `contains` / `åŒ…å«`: A contains B as a component / A åŒ…å« B ä½œä¸ºç»„æˆéƒ¨åˆ†
- `belongs_to` / `å±žäºŽ`: A belongs to category B / A å±žäºŽ B ç±»åˆ«
- `has_property` / `å…·æœ‰å±žæ€§`: A has characteristic or attribute / A å…·æœ‰æŸç§ç‰¹æ€§æˆ–å±žæ€§
- `used_for` / `ç”¨äºŽ`: A is used to achieve/accomplish B / A ç”¨äºŽå®žçŽ°/å®Œæˆ B
- `affects` / `å½±å“`: A affects B / A å¯¹ B äº§ç”Ÿå½±å“
- `relates_to` / `å…³è”`: A is related to B / A ä¸Ž B å­˜åœ¨å…³è”
- `composed_of` / `ç”±...ç»„æˆ`: A is composed of B / A ç”± B ç»„æˆ
- `produces` / `äº§ç”Ÿ`: A produces/generates B / A äº§ç”Ÿ/ç”Ÿæˆ B
- `depends_on` / `ä¾èµ–`: A depends on B / A ä¾èµ–äºŽ B
- `causes` / `å¯¼è‡´`: A causes B / A å¯¼è‡´ B
- `implements` / `å®žçŽ°`: A implements B / A å®žçŽ°äº† B
- `derived_from` / `æ´¾ç”Ÿè‡ª`: A is derived from B / A æ´¾ç”Ÿè‡ª B
- `similar_to` / `ç›¸ä¼¼äºŽ`: A is similar to B / A ä¸Ž B ç›¸ä¼¼

**Output Format (Pure JSON) / è¾“å‡ºæ ¼å¼ï¼ˆçº¯ JSONï¼‰ï¼š**
{{
  "triplets": [
    {{
      "subject": "Concept/Entity Name",
      "predicate": "Relationship Type (use types from guide above)",
      "object": "Target Concept/Entity/Attribute Value",
      "confidence": 0.85,
      "language": "en" or "zh" or "mixed"
    }}
  ]
}}

**Important Notes / æ³¨æ„äº‹é¡¹ï¼š**
- Subject and object should be concise, standardized nouns or noun phrases / ä¸»ä½“å’Œå®¢ä½“åº”è¯¥æ˜¯ç®€æ´ã€è§„èŒƒçš„åè¯æˆ–åè¯çŸ­è¯­
- Keep the original language of concepts (do NOT translate) / ä¿æŒæ¦‚å¿µçš„åŽŸå§‹è¯­è¨€ï¼ˆä¸è¦ç¿»è¯‘ï¼‰
- Use English relationship type identifiers (e.g., "is_a", "contains") / ä½¿ç”¨è‹±æ–‡å…³ç³»ç±»åž‹æ ‡è¯†ç¬¦
- Confidence should reflect knowledge certainty (0.0-1.0) / confidence åº”åæ˜ çŸ¥è¯†ç¡®å®šç¨‹åº¦ï¼ˆ0.0-1.0ï¼‰
- Give higher confidence to definitional, structural knowledge / å¯¹å®šä¹‰æ€§ã€ç»“æž„æ€§çŸ¥è¯†ç»™äºˆæ›´é«˜ç½®ä¿¡åº¦
- Ignore trivial details, focus on core knowledge / å¿½ç•¥æ— å…³ç»†èŠ‚ï¼Œèšç„¦æ ¸å¿ƒçŸ¥è¯†ç‚¹
- If no clear knowledge in text, return empty array / å¦‚æžœæ–‡æœ¬ä¸­æ²¡æœ‰æ˜Žç¡®çŸ¥è¯†ç‚¹ï¼Œè¿”å›žç©ºæ•°ç»„

Return ONLY the JSON object, no other explanatory text.
åªè¿”å›ž JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜Žã€‚"""
    
    def _mock_extract(self, chunk: Chunk) -> List[Triplet]:
        """Mock extraction for testing without OpenAI API."""
        # Simple rule-based extraction as fallback
        triplets = []
        text = chunk.text
        
        # Look for "X is Y" patterns
        import re
        is_pattern = re.compile(r'([A-Z][a-zA-Z\s]+?)\s+is\s+(?:a\s+)?([a-zA-Z\s]+?)(?:\.|,|$)')
        for match in is_pattern.finditer(text):
            triplets.append(Triplet(
                subject=match.group(1).strip(),
                predicate="is_a",
                object=match.group(2).strip(),
                confidence=0.7,
                evidence={
                    "docId": chunk.doc_id,
                    "chunkId": chunk.chunk_id,
                    "page": chunk.meta.get("page"),
                    "offset": chunk.meta.get("offset"),
                    "text": chunk.text[:200]
                },
                doc_id=chunk.doc_id,
                chunk_id=chunk.chunk_id
            ))
        
        return triplets

