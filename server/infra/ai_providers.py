"""AI Provider configuration and unified interface."""
from typing import Optional, Literal, List, Dict, Any
from openai import OpenAI
import anthropic
import json


# æ”¯æŒçš„AIæä¾›å•†ç±»å‹
AIProviderType = Literal[
    "openai",           # OpenAI GPT
    "anthropic",        # Anthropic Claude
    "google",           # Google Gemini
    "deepseek",         # DeepSeek
    "qwen",             # é˜¿é‡Œäº‘é€šä¹‰åƒé—®
    "glm",              # æ™ºè°±AI (GLM)
    "moonshot",         # æœˆä¹‹æš—é¢ Kimi
    "ernie",            # ç™¾åº¦æ–‡å¿ƒä¸€è¨€
    "minimax",          # MiniMax
    "doubao",           # å­—èŠ‚è±†åŒ…
    "ollama",           # Ollama æœ¬åœ°æ¨¡å‹
    "mock"              # Mock æ¨¡å¼ï¼ˆæµ‹è¯•ç”¨ï¼‰
]


class BaseAIClient:
    """Base AI client interface."""
    
    def __init__(self, model: str, **kwargs):
        self.model = model
        self.kwargs = kwargs
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.3,
        **extra_params
    ) -> str:
        """
        Send chat completion request.
        
        Args:
            messages: List of message dicts with 'role' and 'content'
            temperature: Sampling temperature
            extra_params: Additional provider-specific parameters
            
        Returns:
            Response text content
        """
        raise NotImplementedError


class OpenAIClient(BaseAIClient):
    """OpenAI GPT client."""
    
    def __init__(self, api_key: str, model: str, base_url: Optional[str] = None):
        super().__init__(model)
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.3,
        **extra_params
    ) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            response_format={"type": "json_object"} if extra_params.get("json_mode") else None,
            **{k: v for k, v in extra_params.items() if k != "json_mode"}
        )
        return response.choices[0].message.content


class AnthropicClient(BaseAIClient):
    """Anthropic Claude client."""
    
    def __init__(self, api_key: str, model: str, base_url: Optional[str] = None):
        super().__init__(model)
        kwargs = {"api_key": api_key}
        if base_url:
            kwargs["base_url"] = base_url
        self.client = anthropic.Anthropic(**kwargs)
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.3,
        **extra_params
    ) -> str:
        # Anthropic çš„ messages æ ¼å¼ç¨æœ‰ä¸åŒï¼Œéœ€è¦åˆ†ç¦» system æ¶ˆæ¯
        system_msg = None
        user_messages = []
        
        for msg in messages:
            if msg["role"] == "system":
                system_msg = msg["content"]
            else:
                user_messages.append(msg)
        
        # å¦‚æœè¯·æ±‚ JSON æ¨¡å¼ï¼Œåœ¨ system prompt ä¸­æ·»åŠ è¦æ±‚
        if extra_params.get("json_mode"):
            json_instruction = "\n\né‡è¦ï¼šè¯·ç¡®ä¿è¿”å›çš„å†…å®¹æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬æˆ–è¯´æ˜ã€‚"
            if system_msg:
                system_msg = system_msg + json_instruction
            else:
                system_msg = json_instruction.strip()
        
        kwargs = {
            "model": self.model,
            "messages": user_messages,
            "temperature": temperature,
            "max_tokens": extra_params.get("max_tokens", 4096)
        }
        
        # è¿‡æ»¤æ‰ json_mode å‚æ•°
        kwargs = {k: v for k, v in kwargs.items() if k != "json_mode"}
        
        if system_msg:
            kwargs["system"] = system_msg
        
        response = self.client.messages.create(**kwargs)
        return response.content[0].text


class GoogleGeminiClient(BaseAIClient):
    """Google Gemini client (via OpenAI-compatible API)."""
    
    def __init__(self, api_key: str, model: str, base_url: Optional[str] = None):
        super().__init__(model)
        # Google Gemini å¯ä»¥é€šè¿‡ OpenAI å…¼å®¹æ¥å£è®¿é—®
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url or "https://generativelanguage.googleapis.com/v1beta/openai/"
        )
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.3,
        **extra_params
    ) -> str:
        # å¤„ç† json_mode å‚æ•°
        params = {k: v for k, v in extra_params.items() if k != "json_mode"}
        if extra_params.get("json_mode"):
            params["response_format"] = {"type": "json_object"}
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            **params
        )
        return response.choices[0].message.content


class OpenAICompatibleClient(BaseAIClient):
    """
    Generic OpenAI-compatible client for providers like:
    - DeepSeek
    - é€šä¹‰åƒé—® (Qwen)
    - æ™ºè°±AI (GLM)
    - Moonshot (Kimi)
    - æ–‡å¿ƒä¸€è¨€ (ERNIE)
    - MiniMax
    - è±†åŒ… (Doubao)
    - Ollama
    """
    
    def __init__(self, api_key: str, model: str, base_url: str):
        super().__init__(model)
        # è§„èŒƒåŒ– base_urlï¼šç§»é™¤æœ«å°¾æ–œæ ï¼ˆé™¤éæ˜¯ Google çš„ç‰¹æ®Šæƒ…å†µï¼‰
        normalized_base_url = base_url.rstrip('/') if not base_url.endswith('/openai/') else base_url
        
        # ç‰¹æ®Šå¤„ç†ï¼šOllama éœ€è¦ /v1 åç¼€
        # å¦‚æœ base_url æ˜¯ localhost:11434 ä¸”æ²¡æœ‰ /v1ï¼Œè‡ªåŠ¨æ·»åŠ 
        if ':11434' in normalized_base_url and not normalized_base_url.endswith('/v1'):
            normalized_base_url = normalized_base_url + '/v1'
            print(f"âš ï¸  [AIå®¢æˆ·ç«¯] æ£€æµ‹åˆ° Ollama base_urlï¼Œå·²è‡ªåŠ¨æ·»åŠ  /v1 åç¼€")
        
        # éªŒè¯ base_url æ ¼å¼
        if not normalized_base_url.startswith(('http://', 'https://')):
            raise ValueError(f"Invalid base_url format: {base_url}. Must start with http:// or https://")
        
        print(f"ğŸ”— [AIå®¢æˆ·ç«¯] åˆå§‹åŒ– OpenAI å…¼å®¹å®¢æˆ·ç«¯")
        print(f"   Model: {model}")
        print(f"   Base URL: {normalized_base_url}")
        
        self.client = OpenAI(
            api_key=api_key,
            base_url=normalized_base_url
        )
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.3,
        **extra_params
    ) -> str:
        # å¤„ç† json_mode å‚æ•°
        params = {k: v for k, v in extra_params.items() if k != "json_mode"}
        if extra_params.get("json_mode"):
            params["response_format"] = {"type": "json_object"}
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                **params
            )
            return response.choices[0].message.content
        except Exception as e:
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_msg = str(e)
            error_type = type(e).__name__
            
            # è·å–å®é™…çš„ base_urlï¼ˆä»å®¢æˆ·ç«¯å¯¹è±¡ï¼‰
            actual_base_url = getattr(self.client, 'base_url', 'unknown')
            
            if "404" in error_msg or "not found" in error_msg.lower() or error_type == "NotFoundError":
                raise ValueError(
                    f"API endpoint not found (404). "
                    f"Please check your base_url configuration. "
                    f"Current base_url: {actual_base_url}, "
                    f"Model: {self.model}. "
                    f"This usually means:\n"
                    f"  1. The base_url is incorrect or incomplete\n"
                    f"  2. The API endpoint path is wrong\n"
                    f"  3. The service is not available at the specified URL\n"
                    f"Original error: {error_msg}"
                ) from e
            raise


class MockClient(BaseAIClient):
    """Mock client for testing."""
    
    def __init__(self):
        super().__init__("mock")
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.3,
        **extra_params
    ) -> str:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æŒ‡ä»£æ¶ˆè§£è¯·æ±‚ï¼ˆé€šè¿‡ prompt å†…å®¹åˆ¤æ–­ï¼‰
        user_message = ""
        for msg in messages:
            if msg.get("role") == "user":
                user_message = msg.get("content", "")
                break
        
        # å¦‚æœæ˜¯æŒ‡ä»£æ¶ˆè§£è¯·æ±‚ï¼ˆåŒ…å«"æŒ‡ä»£æ¶ˆè§£"å…³é”®è¯ï¼‰
        if "æŒ‡ä»£æ¶ˆè§£" in user_message or "mention" in user_message.lower():
            # å°è¯•ä» prompt ä¸­æå–æåŠä¿¡æ¯
            import re
            # ç®€å•çš„ mock å“åº”ï¼šå‡è®¾ç¬¬ä¸€ä¸ªæåŠåŒ¹é…ç¬¬ä¸€ä¸ªå€™é€‰å…ˆè¡Œè¯
            resolutions = []
            mention_matches = re.findall(r'"mention_text":\s*"([^"]+)"', user_message)
            candidate_matches = re.findall(r'"text":\s*"([^"]+)"', user_message)
            
            # å¦‚æœæœ‰æåŠå’Œå€™é€‰ï¼Œåˆ›å»ºç®€å•çš„æ˜ å°„
            if mention_matches and candidate_matches:
                for i, mention_text in enumerate(mention_matches[:3]):  # æœ€å¤šå¤„ç†å‰3ä¸ª
                    if i < len(candidate_matches):
                        resolutions.append({
                            "mention_id": i + 1,
                            "mention_text": mention_text,
                            "antecedent_text": candidate_matches[0],  # ç®€å•æ˜ å°„åˆ°ç¬¬ä¸€ä¸ªå€™é€‰
                            "confidence": 0.8,
                            "rationale": "Mock æ¨¡å¼ï¼šè‡ªåŠ¨æ˜ å°„"
                        })
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å“åº”
            if not resolutions:
                resolutions = [{
                    "mention_id": 1,
                    "mention_text": "å®ƒ",
                    "antecedent_text": "ç¤ºä¾‹å®ä½“",
                    "confidence": 0.7,
                    "rationale": "Mock æ¨¡å¼ï¼šé»˜è®¤å“åº”"
                }]
            
            return json.dumps({
                "resolutions": resolutions,
                "resolved_text": None  # Mock æ¨¡å¼ä¸ç”Ÿæˆæ›¿æ¢æ–‡æœ¬
            }, ensure_ascii=False)
        
        # é»˜è®¤è¿”å›å®ä½“æŠ½å–æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        return json.dumps({
            "triplets": [
                {
                    "subject": "ç¤ºä¾‹ä¸»ä½“",
                    "predicate": "relates_to",
                    "object": "ç¤ºä¾‹å®¢ä½“",
                    "confidence": 0.5,
                    "language": "zh"
                }
            ]
        }, ensure_ascii=False)


class AIProviderFactory:
    """Factory for creating AI clients."""
    
    # é»˜è®¤çš„ base_url é…ç½®
    # æ³¨æ„ï¼šbase_url ä¸åº”ä»¥æ–œæ ç»“å°¾ï¼ˆé™¤äº† Google çš„ç‰¹æ®Šæƒ…å†µï¼‰
    DEFAULT_BASE_URLS = {
        "openai": None,  # ä½¿ç”¨é»˜è®¤
        "anthropic": None,
        "google": "https://generativelanguage.googleapis.com/v1beta/openai/",  # Google éœ€è¦æœ«å°¾æ–œæ 
        "deepseek": "https://api.deepseek.com/v1",
        "qwen": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "glm": "https://open.bigmodel.cn/api/paas/v4",
        "moonshot": "https://api.moonshot.cn/v1",
        "ernie": "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop",  # æ–‡å¿ƒä¸€è¨€ API
        "minimax": "https://api.minimax.chat/v1",
        "doubao": "https://ark.cn-beijing.volces.com/api/v3",
        "ollama": "http://localhost:11434/v1"
    }
    
    # é»˜è®¤æ¨¡å‹é…ç½®
    DEFAULT_MODELS = {
        "openai": "gpt-4o-mini",
        "anthropic": "claude-3-5-sonnet-20241022",
        "google": "gemini-2.0-flash-exp",
        "deepseek": "deepseek-chat",
        "qwen": "qwen-plus",
        "glm": "glm-4-flash",
        "moonshot": "moonshot-v1-8k",
        "ernie": "ernie-4.0-8k-latest",
        "minimax": "abab6.5s-chat",
        "doubao": "doubao-pro-4k",
        "ollama": "llama3",
        "mock": "mock"
    }
    
    @classmethod
    def create_client(
        cls,
        provider: AIProviderType,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        base_url: Optional[str] = None
    ) -> BaseAIClient:
        """
        Create AI client based on provider type.
        
        Args:
            provider: AI provider type
            api_key: API key (not needed for mock/ollama)
            model: Model name (use default if not provided)
            base_url: Custom base URL (use default if not provided)
            
        Returns:
            AI client instance
            
        Raises:
            ValueError: If provider is not supported or missing required config
        """
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹å’Œbase_urlï¼ˆå¦‚æœæœªæä¾›ï¼‰
        model = model or cls.DEFAULT_MODELS.get(provider)
        base_url = base_url or cls.DEFAULT_BASE_URLS.get(provider)
        
        if provider == "mock":
            return MockClient()
        
        if provider == "openai":
            if not api_key:
                raise ValueError("OpenAI API key is required")
            return OpenAIClient(api_key, model, base_url)
        
        if provider == "anthropic":
            if not api_key:
                raise ValueError("Anthropic API key is required")
            return AnthropicClient(api_key, model, base_url)
        
        if provider == "google":
            if not api_key:
                raise ValueError("Google API key is required")
            return GoogleGeminiClient(api_key, model, base_url)
        
        # å…¶ä»–æ‰€æœ‰ OpenAI å…¼å®¹çš„æä¾›å•†
        if provider in ["deepseek", "qwen", "glm", "moonshot", "ernie", "minimax", "doubao", "ollama"]:
            # Ollama ä¸éœ€è¦çœŸå®çš„ API key
            if provider == "ollama":
                api_key = api_key or "ollama"
            elif not api_key:
                raise ValueError(f"{provider} API key is required")
            
            return OpenAICompatibleClient(api_key, model, base_url)
        
        raise ValueError(f"Unsupported AI provider: {provider}")
    
    @classmethod
    def get_provider_info(cls, provider: AIProviderType) -> Dict[str, Any]:
        """Get provider information including default model and base URL."""
        return {
            "provider": provider,
            "default_model": cls.DEFAULT_MODELS.get(provider),
            "default_base_url": cls.DEFAULT_BASE_URLS.get(provider)
        }
    
    @classmethod
    def list_providers(cls) -> List[Dict[str, Any]]:
        """List all supported providers with their default configurations."""
        return [
            {
                "id": "openai",
                "name": "OpenAI GPT",
                "default_model": cls.DEFAULT_MODELS["openai"],
                "requires_api_key": True
            },
            {
                "id": "anthropic",
                "name": "Anthropic Claude",
                "default_model": cls.DEFAULT_MODELS["anthropic"],
                "requires_api_key": True
            },
            {
                "id": "google",
                "name": "Google Gemini",
                "default_model": cls.DEFAULT_MODELS["google"],
                "requires_api_key": True
            },
            {
                "id": "deepseek",
                "name": "DeepSeek",
                "default_model": cls.DEFAULT_MODELS["deepseek"],
                "requires_api_key": True
            },
            {
                "id": "qwen",
                "name": "é˜¿é‡Œäº‘é€šä¹‰åƒé—®",
                "default_model": cls.DEFAULT_MODELS["qwen"],
                "requires_api_key": True
            },
            {
                "id": "glm",
                "name": "æ™ºè°±AI (GLM)",
                "default_model": cls.DEFAULT_MODELS["glm"],
                "requires_api_key": True
            },
            {
                "id": "moonshot",
                "name": "æœˆä¹‹æš—é¢ Kimi",
                "default_model": cls.DEFAULT_MODELS["moonshot"],
                "requires_api_key": True
            },
            {
                "id": "ernie",
                "name": "ç™¾åº¦æ–‡å¿ƒä¸€è¨€",
                "default_model": cls.DEFAULT_MODELS["ernie"],
                "requires_api_key": True
            },
            {
                "id": "minimax",
                "name": "MiniMax",
                "default_model": cls.DEFAULT_MODELS["minimax"],
                "requires_api_key": True
            },
            {
                "id": "doubao",
                "name": "å­—èŠ‚è±†åŒ…",
                "default_model": cls.DEFAULT_MODELS["doubao"],
                "requires_api_key": True
            },
            {
                "id": "ollama",
                "name": "Ollama",
                "default_model": cls.DEFAULT_MODELS["ollama"],
                "requires_api_key": False
            },
            {
                "id": "mock",
                "name": "Mock (æµ‹è¯•æ¨¡å¼)",
                "default_model": "mock",
                "requires_api_key": False
            }
        ]

