"""Settings API routes."""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, Literal, List, Dict, Any
from infra.config import settings
from infra.ai_providers import AIProviderFactory
import os
import json

router = APIRouter(prefix="/settings", tags=["settings"])


class AISettings(BaseModel):
    """AI configuration settings."""
    ai_provider: Literal[
        "openai", "anthropic", "google", "deepseek", "qwen", 
        "glm", "moonshot", "ernie", "minimax", "doubao", 
        "ollama", "mock"
    ]
    # 通用配置
    ai_api_key: Optional[str] = None
    ai_model: Optional[str] = None
    ai_base_url: Optional[str] = None
    # 向后兼容的旧配置
    openai_api_key: Optional[str] = None
    openai_model: str = "gpt-4o-mini"
    openai_base_url: Optional[str] = None
    ollama_base_url: str = "http://localhost:11434"
    ollama_model: str = "llama3"


class DatabaseSettings(BaseModel):
    """Database configuration settings."""
    neo4j_uri: str
    neo4j_user: str
    redis_url: str


class AllSettings(BaseModel):
    """All application settings."""
    ai_provider: Literal[
        "openai", "anthropic", "google", "deepseek", "qwen", 
        "glm", "moonshot", "ernie", "minimax", "doubao", 
        "ollama", "mock"
    ]
    ai_api_key: Optional[str] = None
    ai_model: Optional[str] = None
    ai_base_url: Optional[str] = None
    openai_api_key: Optional[str] = None
    openai_model: str
    openai_base_url: Optional[str] = None
    ollama_base_url: str
    ollama_model: str
    neo4j_uri: str
    neo4j_user: str
    redis_url: str


def get_env_file_path():
    """Get the path to .env file."""
    return os.path.join(os.getcwd(), ".env")


def read_env_file():
    """Read .env file and return as dict."""
    env_path = get_env_file_path()
    env_vars = {}
    
    if os.path.exists(env_path):
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if '=' in line:
                        key, value = line.split('=', 1)
                        env_vars[key.strip()] = value.strip()
    
    return env_vars


def write_env_file(env_vars):
    """Write env_vars dict to .env file."""
    env_path = get_env_file_path()
    
    with open(env_path, 'w', encoding='utf-8') as f:
        f.write("# LunarInsight Configuration\n")
        f.write("# Generated by Settings API\n\n")
        
        f.write("# Neo4j Configuration\n")
        f.write(f"NEO4J_URI={env_vars.get('NEO4J_URI', 'bolt://localhost:7687')}\n")
        f.write(f"NEO4J_USER={env_vars.get('NEO4J_USER', 'neo4j')}\n")
        f.write(f"NEO4J_PASS={env_vars.get('NEO4J_PASS', 'test1234')}\n\n")
        
        f.write("# Redis Configuration\n")
        f.write(f"REDIS_URL={env_vars.get('REDIS_URL', 'redis://localhost:6379/0')}\n\n")
        
        f.write("# AI Provider Configuration\n")
        f.write(f"AI_PROVIDER={env_vars.get('AI_PROVIDER', 'mock')}\n\n")
        
        f.write("# General AI Configuration (推荐使用)\n")
        if env_vars.get('AI_API_KEY'):
            f.write(f"AI_API_KEY={env_vars.get('AI_API_KEY')}\n")
        if env_vars.get('AI_MODEL'):
            f.write(f"AI_MODEL={env_vars.get('AI_MODEL')}\n")
        if env_vars.get('AI_BASE_URL'):
            f.write(f"AI_BASE_URL={env_vars.get('AI_BASE_URL')}\n")
        f.write("\n")
        
        f.write("# OpenAI Configuration (兼容性配置)\n")
        if env_vars.get('OPENAI_API_KEY'):
            f.write(f"OPENAI_API_KEY={env_vars.get('OPENAI_API_KEY')}\n")
        f.write(f"OPENAI_MODEL={env_vars.get('OPENAI_MODEL', 'gpt-4o-mini')}\n")
        if env_vars.get('OPENAI_BASE_URL'):
            f.write(f"OPENAI_BASE_URL={env_vars.get('OPENAI_BASE_URL')}\n")
        f.write("\n")
        
        f.write("# Ollama Configuration (兼容性配置)\n")
        f.write(f"OLLAMA_BASE_URL={env_vars.get('OLLAMA_BASE_URL', 'http://localhost:11434')}\n")
        f.write(f"OLLAMA_MODEL={env_vars.get('OLLAMA_MODEL', 'llama3')}\n\n")
        
        f.write("# File Upload Configuration\n")
        f.write(f"UPLOAD_DIR={env_vars.get('UPLOAD_DIR', './uploads')}\n\n")
        
        f.write("# API Configuration\n")
        f.write(f"API_HOST={env_vars.get('API_HOST', '0.0.0.0')}\n")
        f.write(f"API_PORT={env_vars.get('API_PORT', '8000')}\n")


@router.get("/ai-providers")
async def list_ai_providers():
    """List all supported AI providers."""
    return {
        "providers": AIProviderFactory.list_providers()
    }


@router.get("/")
async def get_settings():
    """Get current settings."""
    # Read from .env file directly to get the latest values
    env_vars = read_env_file()
    
    return {
        "ai_provider": env_vars.get("AI_PROVIDER", settings.ai_provider),
        "ai_api_key": "***" if env_vars.get("AI_API_KEY") else None,
        "ai_model": env_vars.get("AI_MODEL", settings.ai_model),
        "ai_base_url": env_vars.get("AI_BASE_URL", settings.ai_base_url),
        "openai_api_key": "***" if env_vars.get("OPENAI_API_KEY") else None,
        "openai_model": env_vars.get("OPENAI_MODEL", settings.openai_model),
        "openai_base_url": env_vars.get("OPENAI_BASE_URL", settings.openai_base_url),
        "ollama_base_url": env_vars.get("OLLAMA_BASE_URL", settings.ollama_base_url),
        "ollama_model": env_vars.get("OLLAMA_MODEL", settings.ollama_model),
        "neo4j_uri": env_vars.get("NEO4J_URI", settings.neo4j_uri),
        "neo4j_user": env_vars.get("NEO4J_USER", settings.neo4j_user),
        "redis_url": env_vars.get("REDIS_URL", settings.redis_url),
    }


@router.post("/ai")
async def update_ai_settings(ai_settings: AISettings):
    """Update AI provider settings."""
    try:
        env_vars = read_env_file()
        
        # Update AI settings
        env_vars['AI_PROVIDER'] = ai_settings.ai_provider
        
        # 更新通用配置
        if ai_settings.ai_api_key and ai_settings.ai_api_key != "***":
            env_vars['AI_API_KEY'] = ai_settings.ai_api_key
        if ai_settings.ai_model:
            env_vars['AI_MODEL'] = ai_settings.ai_model
        if ai_settings.ai_base_url:
            env_vars['AI_BASE_URL'] = ai_settings.ai_base_url
        
        # 更新兼容性配置
        if ai_settings.openai_api_key and ai_settings.openai_api_key != "***":
            env_vars['OPENAI_API_KEY'] = ai_settings.openai_api_key
        env_vars['OPENAI_MODEL'] = ai_settings.openai_model
        if ai_settings.openai_base_url:
            env_vars['OPENAI_BASE_URL'] = ai_settings.openai_base_url
        
        env_vars['OLLAMA_BASE_URL'] = ai_settings.ollama_base_url
        env_vars['OLLAMA_MODEL'] = ai_settings.ollama_model
        
        write_env_file(env_vars)
        
        return {
            "success": True,
            "message": "AI settings updated successfully. Please restart the server for changes to take effect."
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to update settings: {str(e)}")


@router.post("/test-connection")
async def test_ai_connection(ai_settings: AISettings):
    """Test AI provider connection."""
    try:
        if ai_settings.ai_provider == "mock":
            return {
                "success": True,
                "message": "Mock 模式 - 无需连接测试"
            }
        
        # 使用 AIProviderFactory 创建客户端进行测试
        try:
            # 优先使用通用配置
            api_key = ai_settings.ai_api_key
            model = ai_settings.ai_model
            base_url = ai_settings.ai_base_url
            
            # 向后兼容：如果使用旧的provider，尝试使用旧配置
            if ai_settings.ai_provider == "openai" and not api_key:
                api_key = ai_settings.openai_api_key
                model = model or ai_settings.openai_model
                base_url = base_url or ai_settings.openai_base_url
            elif ai_settings.ai_provider == "ollama" and not base_url:
                base_url = ai_settings.ollama_base_url
                model = model or ai_settings.ollama_model
            
            # 过滤掉 "***" 占位符
            if api_key == "***":
                api_key = None
            
            client = AIProviderFactory.create_client(
                provider=ai_settings.ai_provider,
                api_key=api_key,
                model=model,
                base_url=base_url
            )
            
            # 简单的测试调用
            response = client.chat_completion(
                messages=[{"role": "user", "content": "Hello"}],
                temperature=0.3,
                max_tokens=5
            )
            
            provider_names = {
                "openai": "OpenAI GPT",
                "anthropic": "Anthropic Claude",
                "google": "Google Gemini",
                "deepseek": "DeepSeek",
                "qwen": "阿里云通义千问",
                "glm": "智谱AI (GLM)",
                "moonshot": "月之暗面 Kimi",
                "ernie": "百度文心一言",
                "minimax": "MiniMax",
                "doubao": "字节豆包",
                "ollama": "Ollama"
            }
            provider_name = provider_names.get(ai_settings.ai_provider, ai_settings.ai_provider)
            
            return {
                "success": True,
                "message": f"成功连接到 {provider_name} (模型: {client.model})"
            }
            
        except ValueError as e:
            return {
                "success": False,
                "message": f"配置错误: {str(e)}"
            }
            
    except Exception as e:
        return {
            "success": False,
            "message": f"连接失败: {str(e)}"
        }


@router.get("/ollama/models")
async def get_ollama_models():
    """Get available Ollama models."""
    try:
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{settings.ollama_base_url}/api/tags")
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                return {
                    "success": True,
                    "models": models
                }
            else:
                return {
                    "success": False,
                    "message": "Failed to fetch Ollama models"
                }
    except Exception as e:
        return {
            "success": False,
            "message": f"Failed to connect to Ollama: {str(e)}"
        }

