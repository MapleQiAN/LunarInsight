"""Settings API routes."""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, Literal
from infra.config import settings
import os
import json

router = APIRouter(prefix="/settings", tags=["settings"])


class AISettings(BaseModel):
    """AI configuration settings."""
    ai_provider: Literal["openai", "ollama", "mock"]
    openai_api_key: Optional[str] = None
    openai_model: str = "gpt-4o-mini"
    openai_base_url: Optional[str] = None
    ollama_base_url: str = "http://localhost:11434"
    ollama_model: str = "llama3"


class DatabaseSettings(BaseModel):
    """Database configuration settings."""
    neo4j_uri: str
    neo4j_user: str
    redis_url: str


class AllSettings(BaseModel):
    """All application settings."""
    ai_provider: Literal["openai", "ollama", "mock"]
    openai_api_key: Optional[str] = None
    openai_model: str
    openai_base_url: Optional[str] = None
    ollama_base_url: str
    ollama_model: str
    neo4j_uri: str
    neo4j_user: str
    redis_url: str


def get_env_file_path():
    """Get the path to .env file."""
    return os.path.join(os.getcwd(), ".env")


def read_env_file():
    """Read .env file and return as dict."""
    env_path = get_env_file_path()
    env_vars = {}
    
    if os.path.exists(env_path):
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if '=' in line:
                        key, value = line.split('=', 1)
                        env_vars[key.strip()] = value.strip()
    
    return env_vars


def write_env_file(env_vars):
    """Write env_vars dict to .env file."""
    env_path = get_env_file_path()
    
    with open(env_path, 'w', encoding='utf-8') as f:
        f.write("# LunarInsight Configuration\n")
        f.write("# Generated by Settings API\n\n")
        
        f.write("# Neo4j Configuration\n")
        f.write(f"NEO4J_URI={env_vars.get('NEO4J_URI', 'bolt://localhost:7687')}\n")
        f.write(f"NEO4J_USER={env_vars.get('NEO4J_USER', 'neo4j')}\n")
        f.write(f"NEO4J_PASS={env_vars.get('NEO4J_PASS', 'test1234')}\n\n")
        
        f.write("# Redis Configuration\n")
        f.write(f"REDIS_URL={env_vars.get('REDIS_URL', 'redis://localhost:6379/0')}\n\n")
        
        f.write("# AI Provider Configuration\n")
        f.write(f"AI_PROVIDER={env_vars.get('AI_PROVIDER', 'mock')}\n\n")
        
        f.write("# OpenAI Configuration\n")
        if env_vars.get('OPENAI_API_KEY'):
            f.write(f"OPENAI_API_KEY={env_vars.get('OPENAI_API_KEY')}\n")
        f.write(f"OPENAI_MODEL={env_vars.get('OPENAI_MODEL', 'gpt-4o-mini')}\n")
        if env_vars.get('OPENAI_BASE_URL'):
            f.write(f"OPENAI_BASE_URL={env_vars.get('OPENAI_BASE_URL')}\n")
        f.write("\n")
        
        f.write("# Ollama Configuration\n")
        f.write(f"OLLAMA_BASE_URL={env_vars.get('OLLAMA_BASE_URL', 'http://localhost:11434')}\n")
        f.write(f"OLLAMA_MODEL={env_vars.get('OLLAMA_MODEL', 'llama3')}\n\n")
        
        f.write("# File Upload Configuration\n")
        f.write(f"UPLOAD_DIR={env_vars.get('UPLOAD_DIR', './uploads')}\n\n")
        
        f.write("# API Configuration\n")
        f.write(f"API_HOST={env_vars.get('API_HOST', '0.0.0.0')}\n")
        f.write(f"API_PORT={env_vars.get('API_PORT', '8000')}\n")


@router.get("/")
async def get_settings():
    """Get current settings."""
    return {
        "ai_provider": settings.ai_provider,
        "openai_api_key": "***" if settings.openai_api_key else None,
        "openai_model": settings.openai_model,
        "openai_base_url": settings.openai_base_url,
        "ollama_base_url": settings.ollama_base_url,
        "ollama_model": settings.ollama_model,
        "neo4j_uri": settings.neo4j_uri,
        "neo4j_user": settings.neo4j_user,
        "redis_url": settings.redis_url,
    }


@router.post("/ai")
async def update_ai_settings(ai_settings: AISettings):
    """Update AI provider settings."""
    try:
        env_vars = read_env_file()
        
        # Update AI settings
        env_vars['AI_PROVIDER'] = ai_settings.ai_provider
        
        if ai_settings.openai_api_key:
            env_vars['OPENAI_API_KEY'] = ai_settings.openai_api_key
        env_vars['OPENAI_MODEL'] = ai_settings.openai_model
        if ai_settings.openai_base_url:
            env_vars['OPENAI_BASE_URL'] = ai_settings.openai_base_url
        
        env_vars['OLLAMA_BASE_URL'] = ai_settings.ollama_base_url
        env_vars['OLLAMA_MODEL'] = ai_settings.ollama_model
        
        write_env_file(env_vars)
        
        return {
            "success": True,
            "message": "AI settings updated successfully. Please restart the server for changes to take effect."
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to update settings: {str(e)}")


@router.post("/test-connection")
async def test_ai_connection(ai_settings: AISettings):
    """Test AI provider connection."""
    try:
        if ai_settings.ai_provider == "openai":
            if not ai_settings.openai_api_key:
                return {
                    "success": False,
                    "message": "OpenAI API key is required"
                }
            
            # Test OpenAI connection
            from openai import OpenAI
            client = OpenAI(
                api_key=ai_settings.openai_api_key,
                base_url=ai_settings.openai_base_url
            )
            
            # Simple test call
            response = client.chat.completions.create(
                model=ai_settings.openai_model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5
            )
            
            return {
                "success": True,
                "message": f"Successfully connected to OpenAI ({ai_settings.openai_model})"
            }
            
        elif ai_settings.ai_provider == "ollama":
            # Test Ollama connection
            from openai import OpenAI
            client = OpenAI(
                base_url=f"{ai_settings.ollama_base_url}/v1",
                api_key="ollama"
            )
            
            # Simple test call
            response = client.chat.completions.create(
                model=ai_settings.ollama_model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5
            )
            
            return {
                "success": True,
                "message": f"Successfully connected to Ollama ({ai_settings.ollama_model})"
            }
        
        else:
            return {
                "success": True,
                "message": "Mock mode - no connection test needed"
            }
            
    except Exception as e:
        return {
            "success": False,
            "message": f"Connection failed: {str(e)}"
        }


@router.get("/ollama/models")
async def get_ollama_models():
    """Get available Ollama models."""
    try:
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{settings.ollama_base_url}/api/tags")
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                return {
                    "success": True,
                    "models": models
                }
            else:
                return {
                    "success": False,
                    "message": "Failed to fetch Ollama models"
                }
    except Exception as e:
        return {
            "success": False,
            "message": f"Failed to connect to Ollama: {str(e)}"
        }

