"""File upload routes."""
import uuid
import hashlib
from pathlib import Path
from typing import Optional

from fastapi import APIRouter, File, HTTPException, UploadFile, BackgroundTasks
from pydantic import BaseModel

from infra.neo4j_client import neo4j_client
from infra.storage import Storage
from services.parser import ParserFactory
from services.extractor import TripletExtractor
from services.linker import EntityLinker
from services.graph_service import GraphService

router = APIRouter(prefix="/uploads", tags=["uploads"])

storage = Storage()
extractor = TripletExtractor()
linker = EntityLinker()
graph_service = GraphService()

# Job storage for tracking processing status
processing_jobs = {}

ALLOWED_EXTENSIONS = {
    ".pdf": "pdf",
    ".md": "md",
    ".markdown": "md",
    ".txt": "txt",
    ".doc": "word",
    ".docx": "word",
}

ALLOWED_MIME_TYPES = {
    "application/pdf",
    "text/markdown",
    "text/x-markdown",
    "text/plain",
    "application/msword",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    "application/octet-stream",  # fallback used by some browsers
}

SUPPORTED_TYPES_LABEL = "PDF, Markdown, TXT, Word (DOC/DOCX)"


def get_document_kind(filename: str) -> str:
    """Determine document kind from filename."""
    ext = Path(filename).suffix.lower()
    return ALLOWED_EXTENSIONS.get(ext, "unknown")


def validate_content_type(content_type: Optional[str]) -> None:
    """Validate uploaded file MIME type."""
    if not content_type:
        return
    if content_type not in ALLOWED_MIME_TYPES:
        raise HTTPException(
            status_code=422,
            detail=(
                f"Unsupported MIME type: {content_type}. "
                f"Allowed types: {SUPPORTED_TYPES_LABEL}"
            ),
        )


@router.post("", response_model=dict)
async def upload_file(file: UploadFile = File(...)):
    """
    Upload a file and create a document record.
    
    Returns:
        {
            "documentId": "...",
            "filename": "...",
            "checksum": "...",
            "status": "uploaded"
        }
    """
    # Read file content
    content = await file.read()
    
    if len(content) == 0:
        raise HTTPException(status_code=400, detail="Empty file")
    
    # Determine document kind
    kind = get_document_kind(file.filename)
    if kind == "unknown":
        raise HTTPException(
            status_code=422,
            detail=(
                f"Unsupported file type: {Path(file.filename).suffix or 'unknown'}. "
                f"Allowed types: {SUPPORTED_TYPES_LABEL}"
            ),
        )

    validate_content_type(file.content_type)
    
    # Save file and get checksum
    file_path, checksum = await storage.save_file(content, file.filename)
    
    # Check if document already exists (by checksum)
    existing_docs = neo4j_client.execute_query(
        "MATCH (d:Document {checksum: $checksum}) RETURN d.id as id LIMIT 1",
        {"checksum": checksum}
    )
    
    if existing_docs:
        return {
            "documentId": existing_docs[0]["id"],
            "filename": file.filename,
            "checksum": checksum,
            "status": "duplicate",
            "message": "Document already exists"
        }
    
    # Create document ID
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    
    # Create document in Neo4j
    neo4j_client.create_document(
        doc_id=doc_id,
        filename=file.filename,
        checksum=checksum,
        kind=kind,
        size=len(content),
        mime=file.content_type,
        meta={"path": file_path}
    )
    
    return {
        "documentId": doc_id,
        "filename": file.filename,
        "checksum": checksum,
        "status": "uploaded",
        "path": file_path
    }


@router.get("/{document_id}")
async def get_document(document_id: str):
    """Get document information."""
    result = neo4j_client.execute_query(
        "MATCH (d:Document {id: $doc_id}) RETURN d",
        {"doc_id": document_id}
    )
    
    if not result:
        raise HTTPException(status_code=404, detail="Document not found")
    
    doc_data = result[0]["d"]
    return doc_data


def process_document_background(doc_id: str, file_path: str, kind: str, job_id: str):
    """Background task for processing document into knowledge graph."""
    processing_jobs[job_id] = {
        "status": "processing",
        "documentId": doc_id,
        "progress": 0,
        "message": "å¼€å§‹å¤„ç†æ–‡æ¡£..."
    }
    
    try:
        print(f"\n{'#'*80}")
        print(f"ğŸš€ [æ–‡æ¡£å¤„ç†] å¼€å§‹å¤„ç†æ–‡æ¡£")
        print(f"   - æ–‡æ¡£ID: {doc_id}")
        print(f"   - æ–‡ä»¶è·¯å¾„: {file_path}")
        print(f"   - æ–‡ä»¶ç±»å‹: {kind}")
        print(f"   - ä»»åŠ¡ID: {job_id}")
        print(f"{'#'*80}\n")
        
        # Step 1: Parse document
        processing_jobs[job_id]["message"] = "æ­£åœ¨è§£ææ–‡æ¡£..."
        processing_jobs[job_id]["progress"] = 10
        
        print(f"ğŸ“– [æ­¥éª¤1] è§£ææ–‡æ¡£...")
        parser = ParserFactory.create_parser(kind)
        full_text, chunks = parser.parse(file_path)
        print(f"âœ… [æ­¥éª¤1] è§£æå®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œæ€»é•¿åº¦ {len(full_text)} å­—ç¬¦")
        
        processing_jobs[job_id]["message"] = f"å·²æå– {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œæ­£åœ¨è¿›è¡ŒçŸ¥è¯†æŠ½å–..."
        processing_jobs[job_id]["progress"] = 30
        
        # Step 2: Extract triplets using AI
        print(f"\nğŸ¤– [æ­¥éª¤2] å¼€å§‹çŸ¥è¯†æŠ½å– (å…± {len(chunks)} ä¸ªæ–‡æœ¬å—)...")
        all_triplets = []
        chunk_triplet_counts = []
        
        for i, chunk in enumerate(chunks, 1):
            print(f"\nğŸ“¦ [æ–‡æœ¬å— {i}/{len(chunks)}] å¤„ç†ä¸­...")
            triplets = extractor.extract(chunk)
            all_triplets.extend(triplets)
            chunk_triplet_counts.append(len(triplets))
            processing_jobs[job_id]["progress"] = 30 + int((i / len(chunks)) * 40)
            processing_jobs[job_id]["message"] = f"æ­£åœ¨æŠ½å–çŸ¥è¯†... ({i}/{len(chunks)})"
        
        print(f"\nğŸ“Š [æ­¥éª¤2] çŸ¥è¯†æŠ½å–å®Œæˆ:")
        print(f"   - æ€»ä¸‰å…ƒç»„æ•°: {len(all_triplets)}")
        print(f"   - å„æ–‡æœ¬å—ä¸‰å…ƒç»„æ•°: {chunk_triplet_counts}")
        print(f"   - å¹³å‡æ¯ä¸ªæ–‡æœ¬å—: {len(all_triplets) / len(chunks) if chunks else 0:.2f} ä¸ªä¸‰å…ƒç»„")
        
        processing_jobs[job_id]["message"] = f"å·²æŠ½å– {len(all_triplets)} ä¸ªçŸ¥è¯†ä¸‰å…ƒç»„ï¼Œæ­£åœ¨é“¾æ¥å®ä½“..."
        processing_jobs[job_id]["progress"] = 70
        
        # Step 3: Link and merge entities
        print(f"\nğŸ”— [æ­¥éª¤3] å¼€å§‹å®ä½“é“¾æ¥å’Œåˆå¹¶...")
        linked_triplets = linker.link_and_merge(all_triplets)
        print(f"âœ… [æ­¥éª¤3] å®ä½“é“¾æ¥å®Œæˆ: {len(linked_triplets)} ä¸ªä¸‰å…ƒç»„")
        
        processing_jobs[job_id]["message"] = "æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±..."
        processing_jobs[job_id]["progress"] = 85
        
        # Step 4: Ingest into Neo4j
        print(f"\nğŸ’¾ [æ­¥éª¤4] å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
        graph_service.ingest_triplets(doc_id, linked_triplets)
        print(f"âœ… [æ­¥éª¤4] çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
        
        # Get graph statistics
        concept_names = set(t.subject for t in linked_triplets) | set(t.object for t in linked_triplets)
        
        print(f"\n{'#'*80}")
        print(f"ğŸ‰ [æ–‡æ¡£å¤„ç†] å¤„ç†å®Œæˆ!")
        print(f"   - æ–‡æœ¬å—æ•°: {len(chunks)}")
        print(f"   - çŸ¥è¯†ä¸‰å…ƒç»„æ•°: {len(linked_triplets)}")
        print(f"   - æ¦‚å¿µæ•°é‡: {len(concept_names)}")
        print(f"   - æ–‡æœ¬æ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
        print(f"{'#'*80}\n")
        
        processing_jobs[job_id]["status"] = "completed"
        processing_jobs[job_id]["progress"] = 100
        processing_jobs[job_id]["message"] = "çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼"
        processing_jobs[job_id]["stats"] = {
            "chunks": len(chunks),
            "triplets": len(linked_triplets),
            "concepts": len(concept_names),
            "textLength": len(full_text)
        }
        
    except Exception as e:
        print(f"\n{'#'*80}")
        print(f"âŒ [æ–‡æ¡£å¤„ç†] å¤„ç†å¤±è´¥!")
        print(f"   - é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        error_trace = traceback.format_exc()
        print(f"   - é”™è¯¯è¯¦æƒ…:\n{error_trace}")
        print(f"{'#'*80}\n")
        
        processing_jobs[job_id]["status"] = "failed"
        processing_jobs[job_id]["message"] = f"å¤„ç†å¤±è´¥: {str(e)}"
        processing_jobs[job_id]["progress"] = 0
        processing_jobs[job_id]["error"] = error_trace


@router.post("/process", response_model=dict)
async def upload_and_process(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    auto_process: bool = True
):
    """
    ä¸€ä½“åŒ–æ¥å£ï¼šä¸Šä¼ æ–‡ä»¶å¹¶è‡ªåŠ¨è¿›è¡ŒçŸ¥è¯†æŠ½å–å’Œå›¾è°±æ„å»ºã€‚
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶
        auto_process: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆé»˜è®¤ Trueï¼‰
        
    Returns:
        {
            "documentId": "...",
            "filename": "...",
            "status": "uploaded" or "processing",
            "jobId": "..." (if auto_process=True)
        }
    """
    # Read file content
    content = await file.read()
    
    if len(content) == 0:
        raise HTTPException(status_code=400, detail="Empty file")
    
    # Determine document kind
    kind = get_document_kind(file.filename)
    if kind == "unknown":
        raise HTTPException(
            status_code=422,
            detail=(
                f"Unsupported file type: {Path(file.filename).suffix or 'unknown'}. "
                f"Allowed types: {SUPPORTED_TYPES_LABEL}"
            ),
        )

    validate_content_type(file.content_type)
    
    # Save file and get checksum
    file_path, checksum = await storage.save_file(content, file.filename)
    
    # Check if document already exists (by checksum)
    existing_docs = neo4j_client.execute_query(
        "MATCH (d:Document {checksum: $checksum}) RETURN d.id as id LIMIT 1",
        {"checksum": checksum}
    )
    
    if existing_docs:
        return {
            "documentId": existing_docs[0]["id"],
            "filename": file.filename,
            "checksum": checksum,
            "status": "duplicate",
            "message": "æ–‡æ¡£å·²å­˜åœ¨"
        }
    
    # Create document ID
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    
    # Create document in Neo4j
    neo4j_client.create_document(
        doc_id=doc_id,
        filename=file.filename,
        checksum=checksum,
        kind=kind,
        size=len(content),
        mime=file.content_type,
        meta={"path": file_path}
    )
    
    response = {
        "documentId": doc_id,
        "filename": file.filename,
        "checksum": checksum,
        "status": "uploaded",
        "path": file_path
    }
    
    # If auto_process is enabled, start background processing
    if auto_process and background_tasks:
        job_id = f"job_{uuid.uuid4().hex[:12]}"
        processing_jobs[job_id] = {
            "status": "queued",
            "documentId": doc_id,
            "progress": 0,
            "message": "ç­‰å¾…å¤„ç†..."
        }
        
        background_tasks.add_task(
            process_document_background,
            doc_id,
            file_path,
            kind,
            job_id
        )
        
        response["status"] = "processing"
        response["jobId"] = job_id
        response["message"] = "æ–‡æ¡£å·²ä¸Šä¼ ï¼Œæ­£åœ¨åå°å¤„ç†..."
    
    return response


@router.get("/status/{job_id}")
async def get_processing_status(job_id: str):
    """
    è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€ã€‚
    
    Returns:
        {
            "status": "queued" | "processing" | "completed" | "failed",
            "documentId": "...",
            "progress": 0-100,
            "message": "...",
            "stats": {...} (if completed)
        }
    """
    if job_id not in processing_jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    
    return processing_jobs[job_id]


# Pydantic models for text and URL uploads
class TextUploadRequest(BaseModel):
    """Request model for text content upload."""
    content: str
    title: Optional[str] = None
    auto_process: bool = True


class URLUploadRequest(BaseModel):
    """Request model for URL content upload."""
    url: str
    title: Optional[str] = None
    auto_process: bool = True


@router.post("/text", response_model=dict)
async def upload_text(
    request: TextUploadRequest,
    background_tasks: BackgroundTasks
):
    """
    ä»æ–‡æœ¬å†…å®¹åˆ›å»ºæ–‡æ¡£å¹¶å¯é€‰åœ°è‡ªåŠ¨å¤„ç†ã€‚
    
    Args:
        content: æ–‡æœ¬å†…å®¹
        title: æ–‡æ¡£æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å‰30ä¸ªå­—ç¬¦ï¼‰
        auto_process: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆé»˜è®¤ Trueï¼‰
        
    Returns:
        {
            "documentId": "...",
            "filename": "...",
            "status": "uploaded" or "processing",
            "jobId": "..." (if auto_process=True)
        }
    """
    content = request.content.strip()
    
    if not content:
        raise HTTPException(status_code=400, detail="æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # Generate title from content if not provided
    title = request.title or content[:30].replace('\n', ' ') + "..."
    filename = f"{title}.txt"
    
    # Calculate checksum
    content_bytes = content.encode('utf-8')
    checksum = hashlib.sha256(content_bytes).hexdigest()
    
    # Check if document already exists
    existing_docs = neo4j_client.execute_query(
        "MATCH (d:Document {checksum: $checksum}) RETURN d.id as id LIMIT 1",
        {"checksum": checksum}
    )
    
    if existing_docs:
        return {
            "documentId": existing_docs[0]["id"],
            "filename": filename,
            "checksum": checksum,
            "status": "duplicate",
            "message": "ç›¸åŒå†…å®¹çš„æ–‡æ¡£å·²å­˜åœ¨"
        }
    
    # Save text as file
    file_path, _ = await storage.save_file(content_bytes, filename)
    
    # Create document ID
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    
    # Create document in Neo4j
    neo4j_client.create_document(
        doc_id=doc_id,
        filename=filename,
        checksum=checksum,
        kind="txt",
        size=len(content_bytes),
        mime="text/plain",
        meta={"path": file_path, "source": "text_input"}
    )
    
    response = {
        "documentId": doc_id,
        "filename": filename,
        "checksum": checksum,
        "status": "uploaded",
        "path": file_path
    }
    
    # Auto-process if enabled
    if request.auto_process:
        job_id = f"job_{uuid.uuid4().hex[:12]}"
        processing_jobs[job_id] = {
            "status": "queued",
            "documentId": doc_id,
            "progress": 0,
            "message": "ç­‰å¾…å¤„ç†..."
        }
        
        background_tasks.add_task(
            process_document_background,
            doc_id,
            file_path,
            "txt",
            job_id
        )
        
        response["status"] = "processing"
        response["jobId"] = job_id
        response["message"] = "æ–‡æœ¬å·²ä¿å­˜ï¼Œæ­£åœ¨åå°å¤„ç†..."
    
    return response


@router.post("/url", response_model=dict)
async def upload_url(
    request: URLUploadRequest,
    background_tasks: BackgroundTasks
):
    """
    ä»ç½‘é¡µURLæŠ“å–å†…å®¹å¹¶åˆ›å»ºæ–‡æ¡£ã€‚
    
    Args:
        url: ç½‘é¡µé“¾æ¥
        title: æ–‡æ¡£æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨URLï¼‰
        auto_process: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆé»˜è®¤ Trueï¼‰
        
    Returns:
        {
            "documentId": "...",
            "filename": "...",
            "status": "uploaded" or "processing",
            "jobId": "..." (if auto_process=True)
        }
    """
    import httpx
    from bs4 import BeautifulSoup
    
    url = request.url.strip()
    
    if not url:
        raise HTTPException(status_code=400, detail="URL ä¸èƒ½ä¸ºç©º")
    
    # Fetch webpage content
    try:
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            response = await client.get(url)
            response.raise_for_status()
            html_content = response.text
    except httpx.HTTPError as e:
        raise HTTPException(
            status_code=400,
            detail=f"æ— æ³•è®¿é—®è¯¥ç½‘é¡µ: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æŠ“å–ç½‘é¡µæ—¶å‡ºé”™: {str(e)}"
        )
    
    # Parse HTML and extract text
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # Get page title
        page_title = soup.title.string if soup.title else None
        title = request.title or page_title or url
        
        # Extract text
        text_content = soup.get_text(separator='\n', strip=True)
        
        # Clean up text
        lines = [line.strip() for line in text_content.split('\n') if line.strip()]
        content = '\n'.join(lines)
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è§£æç½‘é¡µå†…å®¹æ—¶å‡ºé”™: {str(e)}"
        )
    
    if not content or len(content) < 50:
        raise HTTPException(
            status_code=400,
            detail="ç½‘é¡µå†…å®¹è¿‡å°‘æˆ–æ— æ³•æå–æœ‰æ•ˆæ–‡æœ¬"
        )
    
    # Generate filename
    filename = f"{title[:50]}.txt"
    
    # Calculate checksum
    content_bytes = content.encode('utf-8')
    checksum = hashlib.sha256(content_bytes).hexdigest()
    
    # Check if document already exists
    existing_docs = neo4j_client.execute_query(
        "MATCH (d:Document {checksum: $checksum}) RETURN d.id as id LIMIT 1",
        {"checksum": checksum}
    )
    
    if existing_docs:
        return {
            "documentId": existing_docs[0]["id"],
            "filename": filename,
            "checksum": checksum,
            "status": "duplicate",
            "message": "ç›¸åŒå†…å®¹çš„æ–‡æ¡£å·²å­˜åœ¨"
        }
    
    # Save content as file
    file_path, _ = await storage.save_file(content_bytes, filename)
    
    # Create document ID
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    
    # Create document in Neo4j
    neo4j_client.create_document(
        doc_id=doc_id,
        filename=filename,
        checksum=checksum,
        kind="txt",
        size=len(content_bytes),
        mime="text/plain",
        meta={"path": file_path, "source": "url", "original_url": url}
    )
    
    response = {
        "documentId": doc_id,
        "filename": filename,
        "checksum": checksum,
        "status": "uploaded",
        "path": file_path,
        "sourceUrl": url
    }
    
    # Auto-process if enabled
    if request.auto_process:
        job_id = f"job_{uuid.uuid4().hex[:12]}"
        processing_jobs[job_id] = {
            "status": "queued",
            "documentId": doc_id,
            "progress": 0,
            "message": "ç­‰å¾…å¤„ç†..."
        }
        
        background_tasks.add_task(
            process_document_background,
            doc_id,
            file_path,
            "txt",
            job_id
        )
        
        response["status"] = "processing"
        response["jobId"] = job_id
        response["message"] = "ç½‘é¡µå†…å®¹å·²æŠ“å–ï¼Œæ­£åœ¨åå°å¤„ç†..."
    
    return response

