"""File upload routes."""
import uuid
import hashlib
from pathlib import Path
from typing import Optional

from fastapi import APIRouter, File, HTTPException, UploadFile, BackgroundTasks
from pydantic import BaseModel

from infra.neo4j_client import neo4j_client
from infra.storage import Storage
from services.parser import ParserFactory
from services.extractor import TripletExtractor
from services.linker import EntityLinker
from services.graph_service import GraphService
from services.ai_segmenter import AISegmenter
from models.document import AIExtractionRequest
from infra.queue import get_queue

router = APIRouter(prefix="/uploads", tags=["uploads"])

storage = Storage()
extractor = TripletExtractor()
linker = EntityLinker()
graph_service = GraphService()

# Initialize AI segmenter (optional, based on configured AI provider)
try:
    ai_segmenter = AISegmenter()
except ValueError as e:
    ai_segmenter = None
    print(f"âš ï¸  AI segmentation disabled: {str(e)}")

# Initialize Redis queue
queue = get_queue()

# Fallback job storage (used when Redis is not available)
processing_jobs = {}

ALLOWED_EXTENSIONS = {
    ".pdf": "pdf",
    ".md": "md",
    ".markdown": "md",
    ".txt": "txt",
    ".doc": "word",
    ".docx": "word",
}

ALLOWED_MIME_TYPES = {
    "application/pdf",
    "text/markdown",
    "text/x-markdown",
    "text/plain",
    "application/msword",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    "application/octet-stream",  # fallback used by some browsers
}

SUPPORTED_TYPES_LABEL = "PDF, Markdown, TXT, Word (DOC/DOCX)"


def get_document_kind(filename: str) -> str:
    """Determine document kind from filename."""
    ext = Path(filename).suffix.lower()
    return ALLOWED_EXTENSIONS.get(ext, "unknown")


def validate_content_type(content_type: Optional[str]) -> None:
    """Validate uploaded file MIME type."""
    if not content_type:
        return
    if content_type not in ALLOWED_MIME_TYPES:
        raise HTTPException(
            status_code=422,
            detail=(
                f"Unsupported MIME type: {content_type}. "
                f"Allowed types: {SUPPORTED_TYPES_LABEL}"
            ),
        )


@router.post("", response_model=dict)
async def upload_file(file: UploadFile = File(...)):
    """
    Upload a file and create a document record.
    
    Returns:
        {
            "documentId": "...",
            "filename": "...",
            "checksum": "...",
            "status": "uploaded"
        }
    """
    # Read file content
    content = await file.read()
    
    if len(content) == 0:
        raise HTTPException(status_code=400, detail="Empty file")
    
    # Determine document kind
    kind = get_document_kind(file.filename)
    if kind == "unknown":
        raise HTTPException(
            status_code=422,
            detail=(
                f"Unsupported file type: {Path(file.filename).suffix or 'unknown'}. "
                f"Allowed types: {SUPPORTED_TYPES_LABEL}"
            ),
        )

    validate_content_type(file.content_type)
    
    # Save file and get checksum
    file_path, checksum = await storage.save_file(content, file.filename)
    
    # Check if document already exists (by checksum)
    existing_docs = neo4j_client.execute_query(
        "MATCH (d:Document {checksum: $checksum}) RETURN d.id as id LIMIT 1",
        {"checksum": checksum}
    )
    
    if existing_docs:
        return {
            "documentId": existing_docs[0]["id"],
            "filename": file.filename,
            "checksum": checksum,
            "status": "duplicate",
            "message": "Document already exists"
        }
    
    # Create document ID
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    
    # Create document in Neo4j
    neo4j_client.create_document(
        doc_id=doc_id,
        filename=file.filename,
        checksum=checksum,
        kind=kind,
        size=len(content),
        mime=file.content_type,
        meta={"path": file_path}
    )
    
    return {
        "documentId": doc_id,
        "filename": file.filename,
        "checksum": checksum,
        "status": "uploaded",
        "path": file_path
    }


@router.get("/{document_id}")
async def get_document(document_id: str):
    """Get document information."""
    result = neo4j_client.execute_query(
        "MATCH (d:Document {id: $doc_id}) RETURN d",
        {"doc_id": document_id}
    )
    
    if not result:
        raise HTTPException(status_code=404, detail="Document not found")
    
    doc_data = result[0]["d"]
    return doc_data


def _update_upload_status(job_id: str, status: str, progress: int, message: str, **kwargs):
    """Update job status (supports both Redis and fallback storage)."""
    # Try to update via RQ if we're in a worker context
    if queue.is_connected():
        try:
            from rq import get_current_job
            current_job = get_current_job()
            if current_job:
                # We're in a worker, update current job
                meta = current_job.meta or {}
                meta.update({
                    "status": status,
                    "progress": progress,
                    "message": message,
                    **kwargs
                })
                current_job.meta = meta
                current_job.save()
                return
        except Exception:
            pass
        
        # Not in worker context, try to fetch job by ID
        try:
            job = queue.get_job(job_id)
            if job:
                meta = job.meta or {}
                meta.update({
                    "status": status,
                    "progress": progress,
                    "message": message,
                    **kwargs
                })
                job.meta = meta
                job.save()
                return
        except Exception:
            pass
    
    # Fallback to in-memory storage
    if job_id not in processing_jobs:
        processing_jobs[job_id] = {}
    processing_jobs[job_id].update({
        "status": status,
        "progress": progress,
        "message": message,
        **kwargs
    })


def process_document_background(
    doc_id: str, 
    file_path: str, 
    kind: str, 
    job_id: str, 
    chunk_size: int = 2000,
    enable_ai_segmentation: bool = False,
    user_prompt: Optional[str] = None,
    optimize_prompt: bool = True,
    root_topic: Optional[str] = None
):
    """
    Background task for processing document into knowledge graph.
    
    Args:
        doc_id: Document ID
        file_path: Path to document file
        kind: Document type
        job_id: Job ID for tracking
        chunk_size: Maximum characters per chunk (default: 2000)
        enable_ai_segmentation: Enable AI-powered intelligent segmentation
        user_prompt: User-defined analysis prompt
        optimize_prompt: Whether to optimize user prompt with AI
    """
    _update_upload_status(job_id, "processing", 0, "å¼€å§‹å¤„ç†æ–‡æ¡£...", documentId=doc_id)
    
    try:
        print(f"\n{'#'*80}")
        print(f"ğŸš€ [æ–‡æ¡£å¤„ç†] å¼€å§‹å¤„ç†æ–‡æ¡£")
        print(f"   - æ–‡æ¡£ID: {doc_id}")
        print(f"   - æ–‡ä»¶è·¯å¾„: {file_path}")
        print(f"   - æ–‡ä»¶ç±»å‹: {kind}")
        print(f"   - ä»»åŠ¡ID: {job_id}")
        print(f"   - Chunkå¤§å°: {chunk_size} å­—ç¬¦")
        print(f"   - AIæ™ºèƒ½åˆ†è¯: {'å¯ç”¨' if enable_ai_segmentation else 'ç¦ç”¨'}")
        if user_prompt:
            print(f"   - ç”¨æˆ·Prompt: {user_prompt[:100]}...")
        print(f"{'#'*80}\n")
        
        # Step 1: Parse document
        _update_upload_status(job_id, "processing", 10, "æ­£åœ¨è§£ææ–‡æ¡£...", documentId=doc_id)
        
        print(f"ğŸ“– [æ­¥éª¤1] è§£ææ–‡æ¡£ (chunk_size={chunk_size})...")
        parser = ParserFactory.create_parser(kind, chunk_size=chunk_size)
        full_text, chunks = parser.parse(file_path)
        print(f"âœ… [æ­¥éª¤1] è§£æå®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œæ€»é•¿åº¦ {len(full_text)} å­—ç¬¦")
        if chunks:
            avg_chunk_size = sum(len(c.text) for c in chunks) / len(chunks)
            print(f"   - å¹³å‡æ¯ä¸ªæ–‡æœ¬å—: {avg_chunk_size:.0f} å­—ç¬¦")
        
        # AIæ™ºèƒ½åˆ†è¯æ¨¡å¼
        if enable_ai_segmentation and ai_segmenter:
            print(f"\nğŸ§  [AIæ¨¡å¼] å¯ç”¨æ™ºèƒ½çŸ¥è¯†æŠ½å–")
            
            # Step 1.5: Optimize user prompt if provided
            final_prompt = None
            if user_prompt:
                _update_upload_status(job_id, "processing", 15, "æ­£åœ¨ä¼˜åŒ–åˆ†ææç¤ºè¯...", documentId=doc_id)
                
                if optimize_prompt:
                    print(f"ğŸ”§ [Promptä¼˜åŒ–] ä¼˜åŒ–ç”¨æˆ·æç¤ºè¯...")
                    final_prompt = ai_segmenter.optimize_user_prompt(user_prompt)
                else:
                    final_prompt = user_prompt
                    print(f"ğŸ“ [Prompt] ä½¿ç”¨åŸå§‹ç”¨æˆ·æç¤ºè¯")
            
            # Step 2: Analyze document structure
            _update_upload_status(job_id, "processing", 20, "æ­£åœ¨åˆ†ææ–‡æ¡£ç»“æ„...", documentId=doc_id)
            
            print(f"\nğŸ” [æ–‡æ¡£åˆ†æ] åˆ†ææ–‡æ¡£æ•´ä½“ç»“æ„...")
            doc_context = ai_segmenter.analyze_document_structure(chunks, final_prompt)
            print(f"âœ… [æ–‡æ¡£åˆ†æ] å®Œæˆ:")
            print(f"   - ä¸»é¢˜: {', '.join(doc_context.get('themes', []))}")
            print(f"   - é¢†åŸŸ: {', '.join(doc_context.get('domains', []))}")
            print(f"   - å…³é”®æ¦‚å¿µ: {', '.join(doc_context.get('key_concepts', [])[:5])}...")
            
            # Step 3: Extract rich knowledge from each chunk
            _update_upload_status(job_id, "processing", 30, "æ­£åœ¨è¿›è¡Œæ·±åº¦çŸ¥è¯†æŠ½å–...", documentId=doc_id)
            
            print(f"\nğŸ’ [æ·±åº¦æŠ½å–] å¼€å§‹æ™ºèƒ½çŸ¥è¯†æŠ½å– (å…± {len(chunks)} ä¸ªæ–‡æœ¬å—)...")
            all_triplets = []
            all_concepts = []
            all_insights = []
            
            for i, chunk in enumerate(chunks, 1):
                print(f"\nğŸ“¦ [æ–‡æœ¬å— {i}/{len(chunks)}] AIæ·±åº¦åˆ†æä¸­...")
                knowledge = ai_segmenter.extract_rich_knowledge(chunk, doc_context, final_prompt)
                
                # æ”¶é›†ä¸‰å…ƒç»„
                triplets = knowledge.get("triplets", [])
                all_triplets.extend(triplets)
                
                # æ”¶é›†ä¸°å¯Œæ¦‚å¿µ
                concepts = knowledge.get("concepts", [])
                all_concepts.extend(concepts)
                
                # æ”¶é›†æ´å¯Ÿ
                insights = knowledge.get("insights", [])
                all_insights.extend(insights)
                
                print(f"   âœ“ æå–: {len(triplets)} ä¸ªå…³ç³», {len(concepts)} ä¸ªæ¦‚å¿µ, {len(insights)} ä¸ªæ´å¯Ÿ")
                
                progress = 30 + int((i / len(chunks)) * 40)
                _update_upload_status(job_id, "processing", progress, f"AIæ·±åº¦åˆ†æä¸­... ({i}/{len(chunks)})", documentId=doc_id)
            
            print(f"\nğŸ“Š [æ·±åº¦æŠ½å–] å®Œæˆ:")
            print(f"   - æ€»å…³ç³»æ•°: {len(all_triplets)}")
            print(f"   - æ€»æ¦‚å¿µæ•°: {len(all_concepts)}")
            print(f"   - æ€»æ´å¯Ÿæ•°: {len(all_insights)}")
            
            # Step 4: Ingest rich concepts first
            _update_upload_status(job_id, "processing", 75, "æ­£åœ¨æ„å»ºä¸°å¯Œæ¦‚å¿µ...", documentId=doc_id)
            
            print(f"\nğŸ’ [æ¦‚å¿µæ„å»º] å†™å…¥ä¸°å¯Œæ¦‚å¿µä¿¡æ¯...")
            graph_service.ingest_rich_concepts(doc_id, all_concepts, root_topic=root_topic)
            
            # Step 5: Link and merge entities
            _update_upload_status(job_id, "processing", 80, "æ­£åœ¨é“¾æ¥å®ä½“...", documentId=doc_id)
            
            print(f"\nğŸ”— [å®ä½“é“¾æ¥] å¼€å§‹å®ä½“é“¾æ¥å’Œåˆå¹¶...")
            linked_triplets = linker.link_and_merge(all_triplets)
            print(f"âœ… [å®ä½“é“¾æ¥] å®Œæˆ: {len(linked_triplets)} ä¸ªä¸‰å…ƒç»„")
            
            # Step 6: Ingest triplets
            _update_upload_status(job_id, "processing", 90, "æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...", documentId=doc_id)
            
            print(f"\nğŸ’¾ [å›¾è°±æ„å»º] å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            graph_service.ingest_triplets(doc_id, linked_triplets, root_topic=root_topic)
            print(f"âœ… [å›¾è°±æ„å»º] å®Œæˆ")
            
            # Get statistics
            concept_names = set(c["name"] for c in all_concepts)
            
            print(f"\n{'#'*80}")
            print(f"ğŸ‰ [AIæ™ºèƒ½å¤„ç†] å¤„ç†å®Œæˆ!")
            print(f"   - æ–‡æœ¬å—æ•°: {len(chunks)}")
            print(f"   - ä¸°å¯Œæ¦‚å¿µæ•°: {len(all_concepts)}")
            print(f"   - çŸ¥è¯†å…³ç³»æ•°: {len(linked_triplets)}")
            print(f"   - æ·±åº¦æ´å¯Ÿæ•°: {len(all_insights)}")
            print(f"   - æ–‡æœ¬æ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
            if all_insights:
                print(f"\nğŸ’¡ [å…³é”®æ´å¯Ÿ]:")
                for insight in all_insights[:3]:
                    print(f"   â€¢ {insight}")
            print(f"{'#'*80}\n")
            
            stats = {
                "chunks": len(chunks),
                "triplets": len(linked_triplets),
                "concepts": len(concept_names),
                "insights": len(all_insights),
                "textLength": len(full_text),
                "mode": "ai_segmentation"
            }
            result_data = {"stats": stats}
            if all_insights:
                result_data["insights"] = all_insights[:10]  # è¿”å›å‰10æ¡æ´å¯Ÿ
            
            _update_upload_status(job_id, "completed", 100, "AIæ™ºèƒ½åˆ†æå®Œæˆï¼", documentId=doc_id, **result_data)
        
        else:
            # ä¼ ç»Ÿæ¨¡å¼
            _update_upload_status(job_id, "processing", 30, f"å·²æå– {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œæ­£åœ¨è¿›è¡ŒçŸ¥è¯†æŠ½å–...", documentId=doc_id)
            
            # Step 2: Extract triplets using AI
            print(f"\nğŸ¤– [æ­¥éª¤2] å¼€å§‹çŸ¥è¯†æŠ½å– (å…± {len(chunks)} ä¸ªæ–‡æœ¬å—)...")
            all_triplets = []
            chunk_triplet_counts = []
            
            for i, chunk in enumerate(chunks, 1):
                print(f"\nğŸ“¦ [æ–‡æœ¬å— {i}/{len(chunks)}] å¤„ç†ä¸­...")
                triplets = extractor.extract(chunk)
                all_triplets.extend(triplets)
                chunk_triplet_counts.append(len(triplets))
                progress = 30 + int((i / len(chunks)) * 40)
                _update_upload_status(job_id, "processing", progress, f"æ­£åœ¨æŠ½å–çŸ¥è¯†... ({i}/{len(chunks)})", documentId=doc_id)
            
            print(f"\nğŸ“Š [æ­¥éª¤2] çŸ¥è¯†æŠ½å–å®Œæˆ:")
            print(f"   - æ€»ä¸‰å…ƒç»„æ•°: {len(all_triplets)}")
            print(f"   - å„æ–‡æœ¬å—ä¸‰å…ƒç»„æ•°: {chunk_triplet_counts}")
            print(f"   - å¹³å‡æ¯ä¸ªæ–‡æœ¬å—: {len(all_triplets) / len(chunks) if chunks else 0:.2f} ä¸ªä¸‰å…ƒç»„")
            
            _update_upload_status(job_id, "processing", 70, f"å·²æŠ½å– {len(all_triplets)} ä¸ªçŸ¥è¯†ä¸‰å…ƒç»„ï¼Œæ­£åœ¨é“¾æ¥å®ä½“...", documentId=doc_id)
            
            # Step 3: Link and merge entities
            print(f"\nğŸ”— [æ­¥éª¤3] å¼€å§‹å®ä½“é“¾æ¥å’Œåˆå¹¶...")
            linked_triplets = linker.link_and_merge(all_triplets)
            print(f"âœ… [æ­¥éª¤3] å®ä½“é“¾æ¥å®Œæˆ: {len(linked_triplets)} ä¸ªä¸‰å…ƒç»„")
            
            _update_upload_status(job_id, "processing", 85, "æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...", documentId=doc_id)
            
            # Step 4: Ingest into Neo4j
            print(f"\nğŸ’¾ [æ­¥éª¤4] å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            graph_service.ingest_triplets(doc_id, linked_triplets, root_topic=root_topic)
            print(f"âœ… [æ­¥éª¤4] çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
            
            # Get graph statistics
            concept_names = set(t.subject for t in linked_triplets) | set(t.object for t in linked_triplets)
            
            print(f"\n{'#'*80}")
            print(f"ğŸ‰ [æ–‡æ¡£å¤„ç†] å¤„ç†å®Œæˆ!")
            print(f"   - æ–‡æœ¬å—æ•°: {len(chunks)}")
            print(f"   - çŸ¥è¯†ä¸‰å…ƒç»„æ•°: {len(linked_triplets)}")
            print(f"   - æ¦‚å¿µæ•°é‡: {len(concept_names)}")
            print(f"   - æ–‡æœ¬æ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
            print(f"{'#'*80}\n")
            
            stats = {
                "chunks": len(chunks),
                "triplets": len(linked_triplets),
                "concepts": len(concept_names),
                "textLength": len(full_text),
                "mode": "traditional"
            }
            _update_upload_status(job_id, "completed", 100, "çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼", documentId=doc_id, stats=stats)
        
    except Exception as e:
        print(f"\n{'#'*80}")
        print(f"âŒ [æ–‡æ¡£å¤„ç†] å¤„ç†å¤±è´¥!")
        print(f"   - é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        error_trace = traceback.format_exc()
        print(f"   - é”™è¯¯è¯¦æƒ…:\n{error_trace}")
        print(f"{'#'*80}\n")
        
        _update_upload_status(job_id, "failed", 0, f"å¤„ç†å¤±è´¥: {str(e)}", documentId=doc_id, error=error_trace)


@router.post("/process", response_model=dict)
async def upload_and_process(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    auto_process: bool = True,
    chunk_size: int = 2000,
    enable_ai_segmentation: bool = False,
    user_prompt: Optional[str] = None,
    optimize_prompt: bool = True,
    root_topic: Optional[str] = None
):
    """
    ä¸€ä½“åŒ–æ¥å£ï¼šä¸Šä¼ æ–‡ä»¶å¹¶è‡ªåŠ¨è¿›è¡ŒçŸ¥è¯†æŠ½å–å’Œå›¾è°±æ„å»ºã€‚
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶
        auto_process: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆé»˜è®¤ Trueï¼‰
        chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ 2000ï¼Œå»ºè®®èŒƒå›´ï¼š1000-8000ï¼‰
        enable_ai_segmentation: å¯ç”¨AIæ™ºèƒ½åˆ†è¯ï¼ˆé»˜è®¤ Falseï¼‰
        user_prompt: ç”¨æˆ·è‡ªå®šä¹‰åˆ†ææç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        optimize_prompt: æ˜¯å¦ç”¨AIä¼˜åŒ–ç”¨æˆ·æç¤ºè¯ï¼ˆé»˜è®¤ Trueï¼‰
        root_topic: ä¸»é¢˜æ ¹èŠ‚ç‚¹åç§°ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›ï¼Œæ–‡ä»¶å†…å®¹å°†é“¾æ¥åˆ°æ­¤ä¸»é¢˜è€Œä¸æ˜¯æ–‡æ¡£
        
    Returns:
        {
            "documentId": "...",
            "filename": "...",
            "status": "uploaded" or "processing",
            "jobId": "..." (if auto_process=True)
        }
    """
    # Validate chunk_size
    if chunk_size < 100:
        raise HTTPException(status_code=400, detail="chunk_size ä¸èƒ½å°äº 100 å­—ç¬¦")
    if chunk_size > 20000:
        raise HTTPException(status_code=400, detail="chunk_size ä¸èƒ½å¤§äº 20000 å­—ç¬¦ï¼ˆå»ºè®®ä¸è¶…è¿‡ 8000ï¼‰")
    
    # Validate AI segmentation
    if enable_ai_segmentation and not ai_segmenter:
        raise HTTPException(
            status_code=400, 
            detail="AIæ™ºèƒ½åˆ†è¯éœ€è¦é…ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡"
        )
    # Read file content
    content = await file.read()
    
    if len(content) == 0:
        raise HTTPException(status_code=400, detail="Empty file")
    
    # Determine document kind
    kind = get_document_kind(file.filename)
    if kind == "unknown":
        raise HTTPException(
            status_code=422,
            detail=(
                f"Unsupported file type: {Path(file.filename).suffix or 'unknown'}. "
                f"Allowed types: {SUPPORTED_TYPES_LABEL}"
            ),
        )

    validate_content_type(file.content_type)
    
    # Save file and get checksum
    file_path, checksum = await storage.save_file(content, file.filename)
    
    # Check if document already exists (by checksum)
    existing_docs = neo4j_client.execute_query(
        "MATCH (d:Document {checksum: $checksum}) RETURN d.id as id LIMIT 1",
        {"checksum": checksum}
    )
    
    if existing_docs:
        return {
            "documentId": existing_docs[0]["id"],
            "filename": file.filename,
            "checksum": checksum,
            "status": "duplicate",
            "message": "æ–‡æ¡£å·²å­˜åœ¨"
        }
    
    # Create document ID
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    
    # Create document in Neo4j
    neo4j_client.create_document(
        doc_id=doc_id,
        filename=file.filename,
        checksum=checksum,
        kind=kind,
        size=len(content),
        mime=file.content_type,
        meta={"path": file_path}
    )
    
    response = {
        "documentId": doc_id,
        "filename": file.filename,
        "checksum": checksum,
        "status": "uploaded",
        "path": file_path
    }
    
    # If auto_process is enabled, start background processing
    if auto_process and background_tasks:
        job_id = f"job_{uuid.uuid4().hex[:12]}"
        
        # Try to use RQ queue if available
        if queue.is_connected():
            job = queue.enqueue(
                process_document_background,
                doc_id,
                file_path,
                kind,
                job_id,
                chunk_size,
                enable_ai_segmentation,
                user_prompt,
                optimize_prompt,
                root_topic,
                job_timeout='1h'
            )
            
            if job:
                # Initialize job metadata
                job.meta = {
                    "status": "queued",
                    "documentId": doc_id,
                    "progress": 0,
                    "message": "ç­‰å¾…å¤„ç†..."
                }
                job.save()
                response["status"] = "processing"
                response["jobId"] = job.id
                response["message"] = "æ–‡æ¡£å·²ä¸Šä¼ ï¼Œæ­£åœ¨åå°å¤„ç†..."
                return response
        
        # Fallback to BackgroundTasks if Redis is not available
        _update_upload_status(job_id, "queued", 0, "ç­‰å¾…å¤„ç†...", documentId=doc_id)
        
        background_tasks.add_task(
            process_document_background,
            doc_id,
            file_path,
            kind,
            job_id,
            chunk_size,
            enable_ai_segmentation,
            user_prompt,
            optimize_prompt,
            root_topic
        )
        
        response["status"] = "processing"
        response["jobId"] = job_id
        response["message"] = "æ–‡æ¡£å·²ä¸Šä¼ ï¼Œæ­£åœ¨åå°å¤„ç†..."
    
    return response


@router.get("/status/{job_id}")
async def get_processing_status(job_id: str):
    """
    è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€ã€‚
    
    Returns:
        {
            "status": "queued" | "processing" | "completed" | "failed",
            "documentId": "...",
            "progress": 0-100,
            "message": "...",
            "stats": {...} (if completed)
        }
    """
    # Try to get status from Redis first
    if queue.is_connected():
        status = queue.get_job_status(job_id)
        if status.get("status") != "not_found":
            return status
    
    # Fallback to in-memory storage
    if job_id not in processing_jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    
    return processing_jobs[job_id]


# Pydantic models for text and URL uploads
class TextUploadRequest(BaseModel):
    """Request model for text content upload."""
    content: str
    title: Optional[str] = None
    auto_process: bool = True
    chunk_size: int = 2000
    enable_ai_segmentation: bool = False
    user_prompt: Optional[str] = None
    optimize_prompt: bool = True
    root_topic: Optional[str] = None


class URLUploadRequest(BaseModel):
    """Request model for URL content upload."""
    url: str
    title: Optional[str] = None
    auto_process: bool = True
    chunk_size: int = 2000
    enable_ai_segmentation: bool = False
    user_prompt: Optional[str] = None
    optimize_prompt: bool = True
    root_topic: Optional[str] = None


@router.post("/text", response_model=dict)
async def upload_text(
    request: TextUploadRequest,
    background_tasks: BackgroundTasks
):
    """
    ä»æ–‡æœ¬å†…å®¹åˆ›å»ºæ–‡æ¡£å¹¶å¯é€‰åœ°è‡ªåŠ¨å¤„ç†ã€‚
    
    Args:
        content: æ–‡æœ¬å†…å®¹
        title: æ–‡æ¡£æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å‰30ä¸ªå­—ç¬¦ï¼‰
        auto_process: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆé»˜è®¤ Trueï¼‰
        chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ 2000ï¼‰
        enable_ai_segmentation: å¯ç”¨AIæ™ºèƒ½åˆ†è¯ï¼ˆé»˜è®¤ Falseï¼‰
        user_prompt: ç”¨æˆ·è‡ªå®šä¹‰åˆ†ææç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        optimize_prompt: æ˜¯å¦ç”¨AIä¼˜åŒ–ç”¨æˆ·æç¤ºè¯ï¼ˆé»˜è®¤ Trueï¼‰
        
    Returns:
        {
            "documentId": "...",
            "filename": "...",
            "status": "uploaded" or "processing",
            "jobId": "..." (if auto_process=True)
        }
    """
    # Validate chunk_size
    chunk_size = request.chunk_size
    if chunk_size < 100:
        raise HTTPException(status_code=400, detail="chunk_size ä¸èƒ½å°äº 100 å­—ç¬¦")
    if chunk_size > 20000:
        raise HTTPException(status_code=400, detail="chunk_size ä¸èƒ½å¤§äº 20000 å­—ç¬¦ï¼ˆå»ºè®®ä¸è¶…è¿‡ 8000ï¼‰")
    
    # Validate AI segmentation
    if request.enable_ai_segmentation and not ai_segmenter:
        raise HTTPException(
            status_code=400, 
            detail="AIæ™ºèƒ½åˆ†è¯éœ€è¦é…ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡"
        )
    
    content = request.content.strip()
    
    if not content:
        raise HTTPException(status_code=400, detail="æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # Generate title from content if not provided
    title = request.title or content[:30].replace('\n', ' ') + "..."
    filename = f"{title}.txt"
    
    # Calculate checksum
    content_bytes = content.encode('utf-8')
    checksum = hashlib.sha256(content_bytes).hexdigest()
    
    # Check if document already exists
    existing_docs = neo4j_client.execute_query(
        "MATCH (d:Document {checksum: $checksum}) RETURN d.id as id LIMIT 1",
        {"checksum": checksum}
    )
    
    if existing_docs:
        return {
            "documentId": existing_docs[0]["id"],
            "filename": filename,
            "checksum": checksum,
            "status": "duplicate",
            "message": "ç›¸åŒå†…å®¹çš„æ–‡æ¡£å·²å­˜åœ¨"
        }
    
    # Save text as file
    file_path, _ = await storage.save_file(content_bytes, filename)
    
    # Create document ID
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    
    # Create document in Neo4j
    neo4j_client.create_document(
        doc_id=doc_id,
        filename=filename,
        checksum=checksum,
        kind="txt",
        size=len(content_bytes),
        mime="text/plain",
        meta={"path": file_path, "source": "text_input"}
    )
    
    response = {
        "documentId": doc_id,
        "filename": filename,
        "checksum": checksum,
        "status": "uploaded",
        "path": file_path
    }
    
    # Auto-process if enabled
    if request.auto_process:
        job_id = f"job_{uuid.uuid4().hex[:12]}"
        
        # Try to use RQ queue if available
        if queue.is_connected():
            job = queue.enqueue(
                process_document_background,
                doc_id,
                file_path,
                "txt",
                job_id,
                chunk_size,
                request.enable_ai_segmentation,
                request.user_prompt,
                request.optimize_prompt,
                request.root_topic,
                job_timeout='1h'
            )
            
            if job:
                # Initialize job metadata
                job.meta = {
                    "status": "queued",
                    "documentId": doc_id,
                    "progress": 0,
                    "message": "ç­‰å¾…å¤„ç†..."
                }
                job.save()
                response["status"] = "processing"
                response["jobId"] = job.id
                response["message"] = "æ–‡æœ¬å·²ä¿å­˜ï¼Œæ­£åœ¨åå°å¤„ç†..."
                return response
        
        # Fallback to BackgroundTasks if Redis is not available
        _update_upload_status(job_id, "queued", 0, "ç­‰å¾…å¤„ç†...", documentId=doc_id)
        
        background_tasks.add_task(
            process_document_background,
            doc_id,
            file_path,
            "txt",
            job_id,
            chunk_size,
            request.enable_ai_segmentation,
            request.user_prompt,
            request.optimize_prompt,
            request.root_topic
        )
        
        response["status"] = "processing"
        response["jobId"] = job_id
        response["message"] = "æ–‡æœ¬å·²ä¿å­˜ï¼Œæ­£åœ¨åå°å¤„ç†..."
    
    return response


@router.post("/url", response_model=dict)
async def upload_url(
    request: URLUploadRequest,
    background_tasks: BackgroundTasks
):
    """
    ä»ç½‘é¡µURLæŠ“å–å†…å®¹å¹¶åˆ›å»ºæ–‡æ¡£ã€‚
    
    Args:
        url: ç½‘é¡µé“¾æ¥
        title: æ–‡æ¡£æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨URLï¼‰
        auto_process: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆé»˜è®¤ Trueï¼‰
        chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ 2000ï¼‰
        enable_ai_segmentation: å¯ç”¨AIæ™ºèƒ½åˆ†è¯ï¼ˆé»˜è®¤ Falseï¼‰
        user_prompt: ç”¨æˆ·è‡ªå®šä¹‰åˆ†ææç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        optimize_prompt: æ˜¯å¦ç”¨AIä¼˜åŒ–ç”¨æˆ·æç¤ºè¯ï¼ˆé»˜è®¤ Trueï¼‰
        
    Returns:
        {
            "documentId": "...",
            "filename": "...",
            "status": "uploaded" or "processing",
            "jobId": "..." (if auto_process=True)
        }
    """
    # Validate chunk_size
    chunk_size = request.chunk_size
    if chunk_size < 100:
        raise HTTPException(status_code=400, detail="chunk_size ä¸èƒ½å°äº 100 å­—ç¬¦")
    if chunk_size > 20000:
        raise HTTPException(status_code=400, detail="chunk_size ä¸èƒ½å¤§äº 20000 å­—ç¬¦ï¼ˆå»ºè®®ä¸è¶…è¿‡ 8000ï¼‰")
    
    # Validate AI segmentation
    if request.enable_ai_segmentation and not ai_segmenter:
        raise HTTPException(
            status_code=400, 
            detail="AIæ™ºèƒ½åˆ†è¯éœ€è¦é…ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡"
        )
    import httpx
    from bs4 import BeautifulSoup
    
    url = request.url.strip()
    
    if not url:
        raise HTTPException(status_code=400, detail="URL ä¸èƒ½ä¸ºç©º")
    
    # Fetch webpage content
    try:
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            response = await client.get(url)
            response.raise_for_status()
            html_content = response.text
    except httpx.HTTPError as e:
        raise HTTPException(
            status_code=400,
            detail=f"æ— æ³•è®¿é—®è¯¥ç½‘é¡µ: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æŠ“å–ç½‘é¡µæ—¶å‡ºé”™: {str(e)}"
        )
    
    # Parse HTML and extract text
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # Get page title
        page_title = soup.title.string if soup.title else None
        title = request.title or page_title or url
        
        # Extract text
        text_content = soup.get_text(separator='\n', strip=True)
        
        # Clean up text
        lines = [line.strip() for line in text_content.split('\n') if line.strip()]
        content = '\n'.join(lines)
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è§£æç½‘é¡µå†…å®¹æ—¶å‡ºé”™: {str(e)}"
        )
    
    if not content or len(content) < 50:
        raise HTTPException(
            status_code=400,
            detail="ç½‘é¡µå†…å®¹è¿‡å°‘æˆ–æ— æ³•æå–æœ‰æ•ˆæ–‡æœ¬"
        )
    
    # Generate filename
    filename = f"{title[:50]}.txt"
    
    # Calculate checksum
    content_bytes = content.encode('utf-8')
    checksum = hashlib.sha256(content_bytes).hexdigest()
    
    # Check if document already exists
    existing_docs = neo4j_client.execute_query(
        "MATCH (d:Document {checksum: $checksum}) RETURN d.id as id LIMIT 1",
        {"checksum": checksum}
    )
    
    if existing_docs:
        return {
            "documentId": existing_docs[0]["id"],
            "filename": filename,
            "checksum": checksum,
            "status": "duplicate",
            "message": "ç›¸åŒå†…å®¹çš„æ–‡æ¡£å·²å­˜åœ¨"
        }
    
    # Save content as file
    file_path, _ = await storage.save_file(content_bytes, filename)
    
    # Create document ID
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    
    # Create document in Neo4j
    neo4j_client.create_document(
        doc_id=doc_id,
        filename=filename,
        checksum=checksum,
        kind="txt",
        size=len(content_bytes),
        mime="text/plain",
        meta={"path": file_path, "source": "url", "original_url": url}
    )
    
    response = {
        "documentId": doc_id,
        "filename": filename,
        "checksum": checksum,
        "status": "uploaded",
        "path": file_path,
        "sourceUrl": url
    }
    
    # Auto-process if enabled
    if request.auto_process:
        job_id = f"job_{uuid.uuid4().hex[:12]}"
        
        # Try to use RQ queue if available
        if queue.is_connected():
            job = queue.enqueue(
                process_document_background,
                doc_id,
                file_path,
                "txt",
                job_id,
                chunk_size,
                request.enable_ai_segmentation,
                request.user_prompt,
                request.optimize_prompt,
                request.root_topic,
                job_timeout='1h'
            )
            
            if job:
                # Initialize job metadata
                job.meta = {
                    "status": "queued",
                    "documentId": doc_id,
                    "progress": 0,
                    "message": "ç­‰å¾…å¤„ç†..."
                }
                job.save()
                response["status"] = "processing"
                response["jobId"] = job.id
                response["message"] = "ç½‘é¡µå†…å®¹å·²æŠ“å–ï¼Œæ­£åœ¨åå°å¤„ç†..."
                return response
        
        # Fallback to BackgroundTasks if Redis is not available
        _update_upload_status(job_id, "queued", 0, "ç­‰å¾…å¤„ç†...", documentId=doc_id)
        
        background_tasks.add_task(
            process_document_background,
            doc_id,
            file_path,
            "txt",
            job_id,
            chunk_size,
            request.enable_ai_segmentation,
            request.user_prompt,
            request.optimize_prompt,
            request.root_topic
        )
        
        response["status"] = "processing"
        response["jobId"] = job_id
        response["message"] = "ç½‘é¡µå†…å®¹å·²æŠ“å–ï¼Œæ­£åœ¨åå°å¤„ç†..."
    
    return response

