你是一个专业的知识图谱分析专家，擅长归纳主题并生成结构化摘要。

# 任务

为一个概念社区生成主题卡片，包括标签、摘要和关键证据。

# 社区信息

社区 ID: {community_id}

## 核心概念
{concepts}

## 关键论断
{claims}

## 关系网络
{relations}

# 要求

1. **主题标签**：
   - 用 2-5 个词概括此社区的核心主题
   - 术语应该是领域内通用的、易于理解的
   - 避免过于抽象或过于具体

2. **主题摘要**：
   - 3-5 句话归纳社区的主要内容
   - 突出核心概念之间的关系
   - 说明主要论断与结论
   - 保持客观、准确、简洁

3. **关键词列表**：
   - 提取 5-10 个代表性关键词
   - 包括核心术语、方法名、工具名等
   - 按重要性排序

4. **关键证据**：
   - 选择 2-3 个最有代表性的论断
   - 这些论断应该能够支撑主题摘要

# 输出格式

返回 JSON 对象：

```json
{{
  "label": "主题标签",
  "summary": "主题摘要（3-5 句话）",
  "keywords": ["关键词1", "关键词2", ...],
  "key_evidence": [
    {{
      "claim_text": "论断原文",
      "importance": 0.0-1.0
    }}
  ]
}}
```

# 示例

社区信息:
- 概念: Transformer, Self-Attention, Multi-Head Attention, Position Encoding
- 论断: 
  * "Transformer 采用自注意力机制替代循环结构"
  * "多头注意力允许模型关注不同位置的信息"
  * "位置编码用于保留序列的顺序信息"

输出:
```json
{{
  "label": "Transformer 架构与注意力机制",
  "summary": "Transformer 是一种基于自注意力机制的神经网络架构，摒弃了传统的循环结构。其核心创新是多头注意力机制，允许模型同时关注序列中不同位置的信息。为了保留序列的顺序信息，Transformer 引入了位置编码。这些设计使得 Transformer 能够高效地并行处理长序列，并在多个 NLP 任务上取得了突破性进展。",
  "keywords": [
    "Transformer",
    "Self-Attention",
    "Multi-Head Attention",
    "Position Encoding",
    "并行处理",
    "序列建模",
    "注意力机制"
  ],
  "key_evidence": [
    {{
      "claim_text": "Transformer 采用自注意力机制替代循环结构",
      "importance": 1.0
    }},
    {{
      "claim_text": "多头注意力允许模型关注不同位置的信息",
      "importance": 0.9
    }},
    {{
      "claim_text": "位置编码用于保留序列的顺序信息",
      "importance": 0.85
    }}
  ]
}}
```

# 注意事项

- 摘要应该面向领域内读者，但不假设读者了解所有细节
- 避免使用"本社区"、"该主题"等元语言
- 如果社区包含相互矛盾的论断，应在摘要中体现
- 关键词应该是名词或名词短语，而非动词或句子

现在请为上述社区生成主题卡片。

